#!/usr/bin/env python3
"""
RLå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ
"""
import torch
import json
import ast
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, LoraConfig, get_peft_model
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import sys
import os

class RLWorkflowGenerator:
    """ä½¿ç”¨RLè®­ç»ƒçš„Qwen2.5-7Bç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ"""

    def __init__(
        self,
        base_model: str = "Qwen/Qwen2.5-7B-Instruct",
        lora_checkpoint: Optional[str] = None,
        device_ids: List[int] = [2, 3],
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None
    ):
        """
        Args:
            base_model: åŸºåº§æ¨¡å‹è·¯å¾„
            lora_checkpoint: LoRAæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨åŸºåº§æ¨¡å‹ï¼‰
            device_ids: ä½¿ç”¨çš„GPU IDåˆ—è¡¨
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
        """
        self.base_model = base_model
        self.lora_checkpoint = lora_checkpoint
        self.device_ids = device_ids
        self.device = f"cuda:{device_ids[0]}" if torch.cuda.is_available() else "cpu"
        self.config = config or {}

        # è®¾ç½®CUDAè®¾å¤‡
        if torch.cuda.is_available():
            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, device_ids))

        print(f"ğŸ”§ åˆå§‹åŒ–RLå·¥ä½œæµç”Ÿæˆå™¨")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  GPU: {device_ids}")

        # åŠ è½½tokenizer
        print(f"ğŸ“¥ åŠ è½½tokenizer: {base_model}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model,
            trust_remote_code=True
        )

        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model}")
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map={"": self.device},
            trust_remote_code=True
        )

        # åŠ è½½LoRAæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if lora_checkpoint:
            print(f"ğŸ“¥ åŠ è½½LoRAæ£€æŸ¥ç‚¹: {lora_checkpoint}")
            self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)
            self.model.eval()

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)

        print(f"âœ… RLå·¥ä½œæµç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿°
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code.",
                "interface": "programmer(problem: str, analysis: str = 'None') -> dict with keys 'code' and 'output'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "sc_ensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            },
            "Review": {
                "description": "Reviews and provides feedback on a solution.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚ç”ŸæˆJSONæ ¼å¼ï¼ˆprompts + graph_codeï¼‰- å€Ÿé‰´AFlowé£æ ¼"""

        prompt = f"""You must generate EXACTLY ONE valid JSON object. Do not generate examples, explanations, or multiple JSONs.

TASK: Solve this {problem_type} problem: {problem}

OUTPUT FORMAT (JSON):
{{
    "prompts": {{
        "OperatorName": "instruction string to optimize for this specific problem"
    }},
    "graph_code": "Python Workflow class code..."
}}

CRITICAL REQUIREMENTS:
1. The "prompts" field contains instruction strings for each operator you use
2. These prompts will be learned by RL - make them problem-specific and effective
3. The "graph_code" should reference prompts via self.prompts["OperatorName"]

Available Operators:

1. Custom(llm) - Most flexible, for any custom task
   Call: await self.custom(input=str, instruction=str)
   Returns: {{'response': str}}

2. AnswerGenerate(llm) - Step-by-step reasoning (NO instruction parameter!)
   Call: await self.answer_generate(input=str)
   Returns: {{'thought': str, 'answer': str}}

3. Programmer(llm) - Auto-generate and execute Python code
   Call: await self.programmer(problem=str, analysis=str)
   Returns: {{'code': str, 'output': str}}

4. Review(llm) - Reviews and provides feedback
   Call: await self.review(problem=str, solution=str)
   Returns: {{'review_result': str, 'feedback': str}}

5. Revise(llm) - Revises solution based on feedback
   Call: await self.revise(problem=str, solution=str, feedback=str)
   Returns: {{'solution': str}}

EXAMPLE OUTPUT:
{{
    "prompts": {{
        "Custom": "ç”¨ä»£æ•°æ–¹æ³•ä¸€æ­¥æ­¥è§£å†³è¿™ä¸ªæ•°å­¦é—®é¢˜ï¼Œæœ€åç”¨boxedæ ¼å¼ç»™å‡ºç­”æ¡ˆ"
    }},
    "graph_code": "import workspace.{problem_type}.workflows.template.operator as operator\\nfrom scripts.async_llm import create_llm_instance\\nfrom scripts.evaluator import DatasetType\\n\\nclass Workflow:\\n    def __init__(self, name: str, llm_config, dataset: DatasetType):\\n        self.name = name\\n        self.dataset = dataset\\n        self.llm = create_llm_instance(llm_config)\\n        self.custom = operator.Custom(self.llm)\\n        self.prompts = None  # Will be injected at runtime\\n\\n    async def __call__(self, problem: str):\\n        solution = await self.custom(input=problem, instruction=self.prompts['Custom'])\\n        return solution['response'], self.llm.get_usage_summary()['total_cost']"
}}

Generate the JSON:"""

        return prompt

    def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        return_full_output: bool = False
    ) -> Dict:
        """
        ç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµ

        Args:
            problem: é—®é¢˜æ–‡æœ¬
            problem_type: é—®é¢˜ç±»å‹ (math/code/qa)
            temperature: é‡‡æ ·æ¸©åº¦
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            return_full_output: æ˜¯å¦è¿”å›å®Œæ•´è¾“å‡º

        Returns:
            {
                "workflow_code": "Pythonä»£ç ",
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }
        """

        # æ„å»ºæç¤ºè¯
        prompt = self._build_generation_prompt(problem, problem_type)

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        # ç”Ÿæˆ (ä¼˜åŒ–å‚æ•°é˜²æ­¢é‡å¤)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=self.config.get('top_p', 0.95),
                top_k=self.config.get('top_k', 50),
                repetition_penalty=self.config.get('repetition_penalty', 1.2),  # é˜²æ­¢é‡å¤ç”Ÿæˆ
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # è§£æè¾“å‡ºï¼ˆæœŸæœ›JSONæ ¼å¼ï¼‰
        workflow_spec, is_valid, error = self._parse_workflow_output(generated_text, problem_type)

        # è¿”å›å®Œæ•´çš„workflow_specï¼ˆåŒ…å«promptså’Œgraph_codeï¼‰
        result = workflow_spec.copy()
        result.update({
            "valid": is_valid,
            "error": error,
            "metadata": {
                "problem": problem,
                "problem_type": problem_type,
                "temperature": temperature,
                "tokens_generated": outputs.shape[1] - inputs['input_ids'].shape[1]
            }
        })

        if return_full_output:
            result["full_output"] = generated_text
            result["prompt"] = prompt

        return result

    def _parse_workflow_output(self, generated_text: str, problem_type: str) -> Tuple[Dict, bool, Optional[str]]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶éªŒè¯å·¥ä½œæµè§„èŒƒï¼ˆJSONæ ¼å¼ï¼‰"""

        # DEBUG: æ‰“å° Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬
        print(f"\n{'='*60}")
        print(f"ğŸ” DEBUG: Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ (å®Œæ•´):")
        print(f"{'='*60}")
        print(generated_text)
        print(f"{'='*60}\n")

        # ä½¿ç”¨æ‹¬å·è®¡æ•°æ³•æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
        json_start = generated_text.find("{")

        if json_start != -1:
            # ä»ç¬¬ä¸€ä¸ª'{'å¼€å§‹è®¡æ•°ï¼Œæ‰¾åˆ°åŒ¹é…çš„'}'
            bracket_count = 0
            json_end = -1

            for i in range(json_start, len(generated_text)):
                if generated_text[i] == '{':
                    bracket_count += 1
                elif generated_text[i] == '}':
                    bracket_count -= 1
                    if bracket_count == 0:
                        json_end = i
                        break

            if json_end != -1:
                json_text = generated_text[json_start:json_end+1]
                print(f"âœ… ä½¿ç”¨æ‹¬å·è®¡æ•°æ³•æå–JSON (é•¿åº¦: {len(json_text)} å­—ç¬¦)")

        if json_start != -1 and json_end != -1 and json_end > json_start:
            try:
                # è§£æJSON
                workflow_spec = json.loads(json_text)

                # éªŒè¯å¿…éœ€å­—æ®µ
                if "prompts" not in workflow_spec or "graph_code" not in workflow_spec:
                    print(f"âš ï¸  JSONç¼ºå°‘å¿…éœ€å­—æ®µ (prompts/graph_code)ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
                    return self._get_default_workflow(problem_type), False, "Missing required fields in JSON"

                # éªŒè¯graph_codeçš„è¯­æ³•
                try:
                    ast.parse(workflow_spec["graph_code"])
                    is_valid = True
                    error = None
                except SyntaxError as e:
                    print(f"âš ï¸  graph_codeè¯­æ³•é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
                    return self._get_default_workflow(problem_type), False, f"Syntax error in graph_code: {str(e)}"

                print(f"âœ… æˆåŠŸè§£æJSONå·¥ä½œæµè§„èŒƒ")
                print(f"  Prompts: {list(workflow_spec['prompts'].keys())}")
                return workflow_spec, is_valid, error

            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSONè§£æå¤±è´¥: {e}")

        # JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–çº¯ä»£ç ï¼ˆå‘åå…¼å®¹ï¼‰
        print(f"âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆJSONï¼Œå°è¯•æå–çº¯ä»£ç ...")
        code_start = generated_text.find("```python")
        if code_start == -1:
            code_start = generated_text.find("class Workflow:")
            if code_start == -1:
                print(f"âš ï¸  æœªæ‰¾åˆ°ä»£ç ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
                return self._get_default_workflow(problem_type), False, "No valid JSON or code found in output"
            code = generated_text[code_start:]
        else:
            code_start += len("```python\n")
            code_end = generated_text.find("```", code_start)
            code = generated_text[code_start:code_end] if code_end != -1 else generated_text[code_start:]

        code = code.strip()

        # éªŒè¯è¯­æ³•å¹¶åŒ…è£…ä¸ºworkflow_spec
        try:
            ast.parse(code)
            # ä»ä»£ç ä¸­æå–é»˜è®¤promptsï¼ˆç®€åŒ–å¤„ç†ï¼‰
            workflow_spec = {
                "prompts": {"Custom": "Solve this problem step by step."},
                "graph_code": code
            }
            print(f"âœ… ä½¿ç”¨çº¯ä»£ç æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰")
            return workflow_spec, True, None
        except SyntaxError as e:
            print(f"âš ï¸  ä»£ç è¯­æ³•é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
            return self._get_default_workflow(problem_type), False, f"Syntax error: {str(e)}"

    def _get_default_workflow(self, problem_type: str = "math") -> Dict:
        """é»˜è®¤å·¥ä½œæµï¼ˆå½“ç”Ÿæˆå¤±è´¥æ—¶ï¼‰ï¼Œè¿”å›dictæ ¼å¼"""
        graph_code = f"""import workspace.{problem_type}.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)
        self.prompts = None  # Will be injected at runtime

    async def __call__(self, problem: str):
        instruction = self.prompts.get("Custom", "Solve this problem step by step.") if self.prompts else "Solve this problem step by step."
        solution = await self.custom(input=problem, instruction=instruction)
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

        return {
            "prompts": {
                "Custom": "Solve this problem step by step."
            },
            "graph_code": graph_code
        }


def test_generator():
    """æµ‹è¯•ç”Ÿæˆå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•RLå·¥ä½œæµç”Ÿæˆå™¨")
    print("=" * 60)

    # æ³¨æ„ï¼šè¿™éœ€è¦Qwenæ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰ä¸‹è½½ä¼šå¾ˆæ…¢
    generator = RLWorkflowGenerator(
        base_model="Qwen/Qwen2.5-7B-Instruct",
        device_ids=[2, 3],
        operator_descriptions_path="/home/yijia/.claude/11/AFlow/workspace/MATH/workflows/template/operator.json"
    )

    # æµ‹è¯•é—®é¢˜
    test_problem = "What is 15 + 27?"

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    # ç”Ÿæˆå·¥ä½œæµ
    result = generator.generate_workflow(
        problem=test_problem,
        problem_type="math",
        temperature=0.7,
        max_new_tokens=1024
    )

    print(f"\nâœ… ç”Ÿæˆç»“æœ:")
    print(f"  æœ‰æ•ˆæ€§: {result['valid']}")
    if result['error']:
        print(f"  é”™è¯¯: {result['error']}")

    print(f"\nğŸ“„ ç”Ÿæˆçš„å·¥ä½œæµä»£ç :")
    print(result['workflow_code'])


if __name__ == "__main__":
    test_generator()
