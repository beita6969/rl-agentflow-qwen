#!/usr/bin/env python3
"""
GRPOè®­ç»ƒå™¨ - åœ¨çº¿å­¦ä¹ æ¨¡å¼çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
"""
import torch
import torch.nn.functional as F
import asyncio
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import time
import json
import wandb  # âœ¨ æ–°å¢wandbé›†æˆ

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer

from data_manager import DataManager
from rl_workflow_generator import RLWorkflowGenerator
from aflow_executor import AFlowExecutor
from reward_computer import RewardComputer
from gpu_manager import GPUManager


class GRPOTrainer:
    """GRPOè®­ç»ƒå™¨ï¼šåœ¨çº¿å­¦ä¹ æ¨¡å¼"""

    def __init__(self, config_path: str = "config/training.yaml"):
        """
        Args:
            config_path: è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        print("=" * 60)
        print("ğŸš€ åˆå§‹åŒ–GRPOè®­ç»ƒå™¨")
        print("=" * 60)

        # GPUç®¡ç†ï¼ˆä½¿ç”¨ç‰©ç†GPU IDï¼‰
        physical_gpus = self.config.get('physical_gpus', self.config['device_mapping'])
        self.gpu_manager = GPUManager(
            target_gpus=physical_gpus,
            protected_pids=self.config.get('protected_pids', []),
            auto_clean=True
        )

        # éªŒè¯GPUç¯å¢ƒ
        if not self.gpu_manager.verify_environment():
            raise RuntimeError("GPUç¯å¢ƒéªŒè¯å¤±è´¥")

        # âœ¨ åˆå§‹åŒ–wandb
        self._initialize_wandb()

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

        print("=" * 60)
        print("âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)

    def _initialize_wandb(self):
        """åˆå§‹åŒ–wandbç›‘æ§"""
        # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–wandbè®¾ç½®
        wandb_config = self.config.get('wandb', {})

        # è®¾ç½®API key(å¦‚æœæä¾›çš„è¯)
        wandb_api_key = wandb_config.get('api_key', 'b42ca0000cf06f97b05eba34f58823ad5f3122a4')

        # å°è¯•ç™»å½•,å¦‚æœå¤±è´¥åˆ™ä½¿ç”¨offlineæ¨¡å¼
        try:
            if wandb_api_key and len(wandb_api_key) == 40:
                wandb.login(key=wandb_api_key)
                mode = "online"
            else:
                print("âš ï¸  wandb API keyæ— æ•ˆæˆ–æœªæä¾›,ä½¿ç”¨offlineæ¨¡å¼")
                mode = "offline"
        except Exception as e:
            print(f"âš ï¸  wandbç™»å½•å¤±è´¥: {e}, ä½¿ç”¨offlineæ¨¡å¼")
            mode = "offline"

        # åˆå§‹åŒ–wandb run
        wandb.init(
            project=wandb_config.get('project', 'aflow-roll-integration'),
            name=wandb_config.get('run_name', f"grpo-training-{time.strftime('%Y%m%d-%H%M%S')}"),
            mode=mode,  # onlineæˆ–offline
            config={
                # è®­ç»ƒé…ç½®
                "base_model": self.config['base_model'],
                "learning_rate": self.config['learning_rate'],
                "batch_size": self.config['rollout_batch_size'],
                "num_sequences": self.config['num_return_sequences_in_group'],
                "max_steps": self.config['max_steps'],
                "lora_rank": self.config['lora_rank'],
                "lora_alpha": self.config['lora_alpha'],
                # æ•°æ®é…ç½®
                "domain_ratios": self.config['domain_ratios'],
                # å¥–åŠ±é…ç½®
                "reward_weights": self.config.get('reward_weights', {}),
            },
            tags=["grpo", "aflow", "roll", "workflow-generation"],
            notes="GRPO training with improved reward function (ROLL+AgentFlow design)"
        )

        print("\nâœ… wandbåˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å¼: {mode}")
        print(f"  é¡¹ç›®: {wandb.run.project}")
        print(f"  Runåç§°: {wandb.run.name}")
        if mode == "online":
            print(f"  Run URL: {wandb.run.url}")
        else:
            print(f"  ç¦»çº¿æ—¥å¿—: wandb/offline-run-*")

    def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""

        # 1. æ•°æ®ç®¡ç†å™¨
        print("\nğŸ“‚ åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
        self.data_manager = DataManager(
            data_dir=self.config['data_dir'],
            domain_ratios=self.config['domain_ratios']
        )
        self.data_manager.initialize()

        # 2. RLæ¨¡å‹ï¼ˆQwen2.5-7B + LoRAï¼‰
        print("\nğŸ¤– åŠ è½½RLæ¨¡å‹...")
        self._load_rl_model()

        # 3. RLå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆå…±äº«å·²åŠ è½½çš„æ¨¡å‹ï¼‰
        print("\nğŸ”§ åˆå§‹åŒ–å·¥ä½œæµç”Ÿæˆå™¨...")
        self.generator = RLWorkflowGenerator(
            base_model=self.config['base_model'],  # ä¼ é€’è·¯å¾„ç”¨äºåŠ è½½tokenizer
            device_ids=self.config['device_mapping'],
            operator_descriptions_path=self.config.get('aflow_operator_descriptions_path')
        )
        # å…±äº«å·²åŠ è½½çš„æ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        self.generator.model = self.model
        self.generator.tokenizer = self.tokenizer

        # 4. AFlowæ‰§è¡Œå™¨
        print("\nâš™ï¸  åˆå§‹åŒ–AFlowæ‰§è¡Œå™¨...")
        timeout = self.config.get('execution_timeout', 180)  # é»˜è®¤180ç§’
        self.executor = AFlowExecutor(
            llm_config_path=self.config['aflow_config_path'],
            timeout=timeout
        )
        print(f"  æ‰§è¡Œè¶…æ—¶: {timeout}ç§’")

        # 5. å¥–åŠ±è®¡ç®—å™¨
        print("\nğŸ¯ åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨...")
        self.reward_computer = RewardComputer(
            reward_weights=self.config.get('reward_weights')
        )

        # 6. ä¼˜åŒ–å™¨
        print("\nğŸ”¬ åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 0.01)
        )

    def _load_rl_model(self):
        """åŠ è½½RLæ¨¡å‹ï¼ˆQwen2.5-7B + LoRAï¼‰"""
        device = f"cuda:{self.config['device_mapping'][0]}"

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['base_model'],
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½åŸºåº§æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['base_model'],
            torch_dtype=torch.bfloat16 if self.config.get('bf16') else torch.float16,
            device_map={"": device},
            trust_remote_code=True
        )

        # åº”ç”¨LoRA
        if self.config.get('use_lora', True):
            lora_config = LoraConfig(
                r=self.config['lora_rank'],
                lora_alpha=self.config['lora_alpha'],
                target_modules=self.config['lora_target_modules'].split(','),
                lora_dropout=self.config['lora_dropout'],
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)

            print(f"âœ… LoRAåº”ç”¨å®Œæˆ")
            self.model.print_trainable_parameters()

    async def train_step(self, step: int) -> Dict:
        """
        å•æ­¥GRPOè®­ç»ƒï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰

        Returns:
            metrics: è®­ç»ƒæŒ‡æ ‡
        """

        # 1. é‡‡æ ·batch
        batch = self.data_manager.sample_batch(
            batch_size=self.config['rollout_batch_size'],
            split="train"
        )

        # ç»Ÿè®¡
        batch_stats = self.data_manager.get_batch_stats(batch)
        print(f"\nğŸ“¦ Batch {step}: {len(batch)} æ ·æœ¬, åˆ†å¸ƒ: {batch_stats}")

        # 2. ä¸ºæ¯ä¸ªé—®é¢˜ç”ŸæˆKä¸ªå·¥ä½œæµï¼ˆGRPOç»„ï¼‰
        all_workflows = []
        all_problems = []
        all_answers = []
        all_rewards = []
        all_log_probs = []

        # âœ¨ æ–°å¢ï¼šå‡†ç¡®ç‡ç»Ÿè®¡
        correctness_scores = []  # å­˜å‚¨æ‰€æœ‰æ­£ç¡®æ€§åˆ†æ•°

        num_sequences = self.config['num_return_sequences_in_group']

        for sample in tqdm(batch, desc="ç”Ÿæˆå’Œæ‰§è¡Œå·¥ä½œæµ"):
            problem = sample['problem']
            ground_truth = sample['ground_truth']
            problem_type = sample['problem_type']

            # GRPOç»„
            group_workflows = []
            group_answers = []
            group_rewards = []
            group_log_probs = []

            for i in range(num_sequences):
                # ç”Ÿæˆå·¥ä½œæµ
                result = self.generator.generate_workflow(
                    problem=problem,
                    problem_type=problem_type,
                    temperature=self.config['generation_config']['temperature']
                )

                workflow_code = result['workflow_code']

                # è®¡ç®—logæ¦‚ç‡ï¼ˆæ—§ç­–ç•¥ï¼‰
                log_prob = await self._compute_log_prob(problem, workflow_code, problem_type)

                # æ‰§è¡Œå·¥ä½œæµ
                try:
                    answer, cost, metadata = await self.executor.execute_workflow(
                        workflow_code=workflow_code,
                        problem=problem,
                        problem_type=problem_type,
                        entry_point=sample.get('entry_point', '')
                    )

                    # è®¡ç®—å¥–åŠ±
                    if metadata['success']:
                        reward = self.reward_computer.compute_reward(
                            problem=problem,
                            prediction=answer,
                            ground_truth=ground_truth,
                            problem_type=problem_type,
                            metadata=metadata
                        )

                        # âœ¨ æ–°å¢ï¼šæ˜¾å¼è®¡ç®—å¹¶è®°å½•æ­£ç¡®æ€§
                        correctness = self.reward_computer._compute_correctness_reward(
                            prediction=answer,
                            ground_truth=ground_truth,
                            problem_type=problem_type
                        )
                        correctness_scores.append(correctness)

                        # åˆ¤æ–­æ˜¯å¦æ­£ç¡®ï¼ˆcorrectness > 5.0 è¡¨ç¤ºæ¥è¿‘æ­£ç¡®æˆ–å®Œå…¨æ­£ç¡®ï¼‰
                        is_correct = correctness >= 5.0
                        status_icon = "âœ…" if is_correct else "âŒ"

                        print(f"  {status_icon} æ­£ç¡®æ€§è¯„åˆ†: {correctness:.1f}/10.0 | é¢„æµ‹: {str(answer)[:50]} | çœŸå€¼: {str(ground_truth)[:50]}")
                    else:
                        reward = -10.0  # æ‰§è¡Œå¤±è´¥æƒ©ç½š
                        correctness_scores.append(-10.0)
                        print(f"  âŒ æ‰§è¡Œå¤±è´¥ | çœŸå€¼: {str(ground_truth)[:50]}")

                except Exception as e:
                    print(f"  âš ï¸  æ‰§è¡Œé”™è¯¯: {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
                    answer = None
                    reward = -10.0
                    correctness_scores.append(-10.0)

                group_workflows.append(workflow_code)
                group_answers.append(answer)
                group_rewards.append(reward)
                group_log_probs.append(log_prob)

            # GRPOå…³é”®ï¼šç»„å†…å¥–åŠ±å½’ä¸€åŒ–
            mean_reward = np.mean(group_rewards)
            group_advantages = [r - mean_reward for r in group_rewards]

            # æ”¶é›†
            all_workflows.extend(group_workflows)
            all_problems.extend([problem] * num_sequences)
            all_answers.extend(group_answers)
            all_rewards.extend(group_advantages)  # ä½¿ç”¨ä¼˜åŠ¿
            all_log_probs.extend(group_log_probs)

        # 3. ç­–ç•¥æ¢¯åº¦æ›´æ–°
        print(f"\nğŸ”„ æ›´æ–°ç­–ç•¥...")
        loss, kl_div = await self._update_policy(
            problems=all_problems,
            workflows=all_workflows,
            old_log_probs=all_log_probs,
            advantages=all_rewards,
            problem_types=[s['problem_type'] for s in batch for _ in range(num_sequences)]
        )

        # 4. æŒ‡æ ‡
        # âœ¨ æ–°å¢ï¼šè®¡ç®—å‡†ç¡®ç‡ç»Ÿè®¡
        num_correct = sum(1 for score in correctness_scores if score >= 5.0)
        num_total = len(correctness_scores)
        accuracy = (num_correct / num_total * 100) if num_total > 0 else 0.0
        avg_correctness = np.mean(correctness_scores) if correctness_scores else 0.0

        metrics = {
            "step": step,
            "loss": loss,
            "kl_div": kl_div,
            "avg_reward": np.mean(all_rewards),
            "max_reward": np.max(all_rewards),
            "min_reward": np.min(all_rewards),
            "num_samples": len(all_workflows),
            # âœ¨ æ–°å¢å‡†ç¡®ç‡æŒ‡æ ‡
            "accuracy": accuracy,
            "num_correct": num_correct,
            "num_total": num_total,
            "avg_correctness_score": avg_correctness
        }

        print(f"\nğŸ¯ å‡†ç¡®ç‡ç»Ÿè®¡: {num_correct}/{num_total} = {accuracy:.1f}% (å¹³å‡æ­£ç¡®æ€§è¯„åˆ†: {avg_correctness:.2f}/10.0)")

        # âœ¨ wandb logging
        wandb.log({
            "train/loss": loss,
            "train/kl_div": kl_div,
            "train/avg_reward": np.mean(all_rewards),
            "train/max_reward": np.max(all_rewards),
            "train/min_reward": np.min(all_rewards),
            "train/accuracy": accuracy,
            "train/avg_correctness_score": avg_correctness,
            "train/num_correct": num_correct,
            "train/num_total": num_total,
            "step": step
        })

        return metrics

    async def _compute_log_prob(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®—å·¥ä½œæµçš„logæ¦‚ç‡ï¼ˆæ—§ç­–ç•¥ï¼‰"""

        self.model.eval()

        with torch.no_grad():
            # æ„å»ºå®Œæ•´æ–‡æœ¬
            prompt = self.generator._build_generation_prompt(problem, problem_type)
            full_text = prompt + workflow_code

            # Tokenize
            inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(**inputs, labels=inputs["input_ids"])

            # è´Ÿå¯¹æ•°ä¼¼ç„¶ -> logæ¦‚ç‡
            log_prob = -outputs.loss

            return log_prob.detach().cpu()

    async def _update_policy(
        self,
        problems: List[str],
        workflows: List[str],
        old_log_probs: List[torch.Tensor],
        advantages: List[float],
        problem_types: List[str]
    ) -> Tuple[float, float]:
        """æ›´æ–°ç­–ç•¥ï¼ˆGRPOï¼‰"""

        self.model.train()

        total_loss = 0.0
        total_kl = 0.0
        num_updates = 0

        # æ¢¯åº¦ç´¯ç§¯
        grad_accum_steps = self.config.get('gradient_accumulation_steps', 1)

        for i in range(0, len(workflows), grad_accum_steps):
            batch_slice = slice(i, min(i + grad_accum_steps, len(workflows)))

            batch_loss = 0.0
            batch_kl = 0.0

            for j in range(i, min(i + grad_accum_steps, len(workflows))):
                problem = problems[j]
                workflow = workflows[j]
                old_log_prob = old_log_probs[j]
                advantage = advantages[j]
                problem_type = problem_types[j]

                # è®¡ç®—æ–°logæ¦‚ç‡
                new_log_prob = await self._compute_log_prob_trainable(problem, workflow, problem_type)

                # é‡è¦æ€§é‡‡æ ·æ¯”
                ratio = torch.exp(new_log_prob - old_log_prob.to(self.model.device))

                # PPOè£å‰ªæŸå¤±
                clip_range = self.config['clip_range']
                clipped_ratio = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)

                advantage_tensor = torch.tensor(advantage, device=self.model.device)
                policy_loss = -torch.min(
                    ratio * advantage_tensor,
                    clipped_ratio * advantage_tensor
                )

                # KLæ­£åˆ™åŒ–
                if self.config.get('use_kl_loss'):
                    kl_loss = self.config['kl_loss_coef'] * (new_log_prob - old_log_prob.to(self.model.device)).pow(2)
                else:
                    kl_loss = 0.0

                # æ€»æŸå¤±
                loss = policy_loss + kl_loss

                # ç´¯ç§¯
                batch_loss += loss
                batch_kl += kl_loss if isinstance(kl_loss, torch.Tensor) else 0.0

            # å¹³å‡
            batch_loss = batch_loss / min(grad_accum_steps, len(workflows) - i)

            # åå‘ä¼ æ’­
            batch_loss.backward()

            total_loss += batch_loss.item()
            total_kl += batch_kl.item() if isinstance(batch_kl, torch.Tensor) else batch_kl
            num_updates += 1

            # ä¼˜åŒ–å™¨æ­¥éª¤
            if (i + grad_accum_steps) % grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.get('max_grad_norm', 1.0))
                self.optimizer.step()
                self.optimizer.zero_grad()

        avg_loss = total_loss / max(num_updates, 1)
        avg_kl = total_kl / max(num_updates, 1)

        return avg_loss, avg_kl

    async def _compute_log_prob_trainable(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®—å·¥ä½œæµçš„logæ¦‚ç‡ï¼ˆæ–°ç­–ç•¥ï¼Œå¯è®­ç»ƒï¼‰"""

        # æ„å»ºå®Œæ•´æ–‡æœ¬
        prompt = self.generator._build_generation_prompt(problem, problem_type)
        full_text = prompt + workflow_code

        # Tokenize
        inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)

        # å‰å‘ä¼ æ’­
        outputs = self.model(**inputs, labels=inputs["input_ids"])

        # è´Ÿå¯¹æ•°ä¼¼ç„¶ -> logæ¦‚ç‡
        log_prob = -outputs.loss

        return log_prob

    async def train(self):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("ğŸ“ å¼€å§‹GRPOè®­ç»ƒ")
        print("=" * 60)

        max_steps = self.config['max_steps']
        save_every = self.config.get('save_every', 50)
        log_every = self.config.get('log_every', 5)

        for step in range(1, max_steps + 1):
            print(f"\n{'=' * 60}")
            print(f"ğŸ“ Step {step}/{max_steps}")
            print(f"{'=' * 60}")

            # è®­ç»ƒæ­¥éª¤
            metrics = await self.train_step(step)

            # æ—¥å¿—
            if step % log_every == 0:
                print(f"\nğŸ“Š Metrics:")
                for key, value in metrics.items():
                    print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % save_every == 0:
                self.save_checkpoint(step)

        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆ")
        print("=" * 60)

    def save_checkpoint(self, step: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path(self.config['output_dir']) / f"step_{step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜LoRAæƒé‡
        self.model.save_pretrained(checkpoint_dir)

        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")


async def main():
    """ä¸»å‡½æ•°"""
    trainer = GRPOTrainer(config_path="config/training.yaml")
    await trainer.train()


if __name__ == "__main__":
    asyncio.run(main())
