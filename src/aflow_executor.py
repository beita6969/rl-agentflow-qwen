#!/usr/bin/env python3
"""
AFlowæ‰§è¡Œé€‚é…å™¨ - æ‰§è¡ŒRLç”Ÿæˆçš„å·¥ä½œæµ
"""
import sys
import os
import tempfile
import importlib.util
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import asyncio
import time

# æ·»åŠ AFlowåˆ°è·¯å¾„ï¼ˆæ·»åŠ å¤šä¸ªå¯èƒ½éœ€è¦çš„è·¯å¾„ï¼‰
aflow_path = '/home/yijia/.claude/11/AFlow'
sys.path.insert(0, aflow_path)
sys.path.insert(0, os.path.join(aflow_path, 'workspace'))

# å¯¼å…¥AFlowç»„ä»¶
from scripts.async_llm import create_llm_instance, LLMsConfig
from scripts import operators as operator_module

class AFlowExecutor:
    """æ‰§è¡ŒRLç”Ÿæˆçš„å·¥ä½œæµï¼Œä½¿ç”¨AFlowçš„ç®—å­"""

    def __init__(
        self,
        llm_config_path: str = "config/aflow_llm.yaml",
        llm_model_name: str = "gpt-4o-mini",
        timeout: int = 300
    ):
        """
        Args:
            llm_config_path: AFlow LLMé…ç½®æ–‡ä»¶è·¯å¾„
            llm_model_name: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            timeout: æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.llm_config_path = Path(llm_config_path)
        self.llm_model_name = llm_model_name
        self.timeout = timeout

        # åŠ è½½LLMé…ç½®
        self._load_llm_config()

        print(f"âœ… AFlowæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  LLMæ¨¡å‹: {llm_model_name}")
        print(f"  è¶…æ—¶: {timeout}ç§’")

    def _load_llm_config(self):
        """åŠ è½½LLMé…ç½®"""
        try:
            # è®¾ç½®é…ç½®è·¯å¾„
            abs_config_path = self.llm_config_path.absolute()

            # è¯»å–YAMLé…ç½®æ–‡ä»¶
            import yaml
            with open(abs_config_path, 'r') as f:
                yaml_data = yaml.safe_load(f)

            # LLMsConfigæœŸæœ›çš„æ˜¯modelså­—å…¸
            models_config = yaml_data.get('models', {})

            # ç›´æ¥åŠ è½½é…ç½®
            from scripts.async_llm import LLMsConfig
            self.llm_configs = LLMsConfig(models_config)

            print(f"âœ… åŠ è½½LLMé…ç½®: {abs_config_path}")

        except Exception as e:
            print(f"âš ï¸  åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
            print(f"  å°†ä½¿ç”¨ LLMsConfig.default()")
            # ä½¿ç”¨é»˜è®¤é…ç½®è€Œä¸æ˜¯ None
            from scripts.async_llm import LLMsConfig
            try:
                self.llm_configs = LLMsConfig.default()
                print(f"âœ… æˆåŠŸåŠ è½½é»˜è®¤LLMé…ç½®")
            except Exception as e2:
                print(f"  é»˜è®¤é…ç½®ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šè®¾ä¸º Noneï¼Œåç»­ç”¨å­—ç¬¦ä¸²
                self.llm_configs = None

    async def execute_workflow(
        self,
        workflow_code: str,
        problem: str,
        problem_type: str = "math",
        **kwargs
    ) -> Tuple[Any, float, Dict]:
        """
        æ‰§è¡Œå·¥ä½œæµ

        Args:
            workflow_code: RLæ¨¡å‹ç”Ÿæˆçš„Workflowç±»ä»£ç 
            problem: é—®é¢˜æ–‡æœ¬
            problem_type: é—®é¢˜ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚entry_point for codeï¼‰

        Returns:
            (answer, cost, metadata)
        """

        start_time = time.time()

        try:
            # åˆ›å»ºä¸´æ—¶å·¥ä½œæµæ¨¡å—
            workflow_class = self._create_workflow_class(workflow_code, problem_type)

            # å®ä¾‹åŒ–å·¥ä½œæµ
            llm_config = self._get_llm_config()

            # ç¡®ä¿ llm_config ä¸æ˜¯ None
            if llm_config is None:
                print(f"âš ï¸  llm_config ä¸º Noneï¼Œé™çº§ä¸ºå­—ç¬¦ä¸²: {self.llm_model_name}")
                llm_config = self.llm_model_name

            try:
                workflow = workflow_class(
                    name="rl_generated_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )
            except Exception as e:
                # å·¥ä½œæµå®ä¾‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨fallback
                print(f"âš ï¸  å·¥ä½œæµå®ä¾‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print(f"  ä½¿ç”¨fallbackå·¥ä½œæµ")
                fallback_class = self._get_fallback_workflow_class(problem_type)
                workflow = fallback_class(
                    name="fallback_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )

            # æ‰§è¡Œï¼ˆå¸¦è¶…æ—¶ï¼‰
            # å°è¯•ä¼ å…¥entry_pointï¼ˆcodeé—®é¢˜éœ€è¦ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™é™çº§ä¸ºåªä¼ problem
            try:
                if problem_type == "code" and "entry_point" in kwargs:
                    try:
                        result = await asyncio.wait_for(
                            workflow(problem, kwargs["entry_point"]),
                            timeout=self.timeout
                        )
                    except TypeError as e:
                        # å¦‚æœWorkflowä¸æ¥å—entry_pointå‚æ•°ï¼Œåªä¼ problem
                        if "positional argument" in str(e):
                            print(f"  âš ï¸  Workflowä¸æ”¯æŒentry_pointå‚æ•°ï¼Œé™çº§ä¸ºåªä¼ problem")
                            result = await asyncio.wait_for(
                                workflow(problem),
                                timeout=self.timeout
                            )
                        else:
                            raise
                else:
                    result = await asyncio.wait_for(
                        workflow(problem),
                        timeout=self.timeout
                    )
            except (AttributeError, TypeError, KeyError, IndexError, ValueError, NameError) as e:
                # è¿è¡Œæ—¶é”™è¯¯ï¼šWorkflowç»“æ„æœ‰é—®é¢˜ï¼Œä½¿ç”¨fallback workflow
                print(f"  âš ï¸  æ‰§è¡Œé”™è¯¯: {type(e).__name__}: {e}")
                print(f"  ä½¿ç”¨fallbackå·¥ä½œæµé‡è¯•")

                # åˆ›å»ºå¹¶ä½¿ç”¨fallback workflow
                fallback_class = self._get_fallback_workflow_class(problem_type)
                fallback_workflow = fallback_class(
                    name="fallback_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )

                result = await asyncio.wait_for(
                    fallback_workflow(problem),
                    timeout=self.timeout
                )

            # å®‰å…¨åœ°è§£åŒ…ç»“æœï¼ˆå¯èƒ½è¿”å›2ä¸ªæˆ–æ›´å¤šå€¼ï¼‰
            if isinstance(result, tuple):
                if len(result) >= 2:
                    answer, cost = result[0], result[1]
                elif len(result) == 1:
                    answer, cost = result[0], 0.0
                else:
                    answer, cost = None, 0.0
            else:
                answer, cost = result, 0.0

            execution_time = time.time() - start_time

            # å…ƒæ•°æ®
            metadata = {
                "success": True,
                "execution_time": execution_time,
                "cost": cost,
                "problem_type": problem_type
            }

            return answer, cost, metadata

        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            print(f"â±ï¸  æ‰§è¡Œè¶…æ—¶ ({self.timeout}ç§’)")

            metadata = {
                "success": False,
                "error": "timeout",
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type
            }

            return None, 0.0, metadata

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ æ‰§è¡Œé”™è¯¯: {str(e)}")

            import traceback
            traceback.print_exc()

            metadata = {
                "success": False,
                "error": str(e),
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type
            }

            return None, 0.0, metadata

    def _create_workflow_class(self, workflow_code: str, problem_type: str):
        """ä»å·¥ä½œæµä»£ç åŠ¨æ€åˆ›å»ºWorkflowç±»"""

        # å‡†å¤‡å‘½åç©ºé—´
        namespace = {
            "operator": operator_module,
            "create_llm_instance": create_llm_instance,
            "DatasetType": str
        }

        # æ›¿æ¢importè·¯å¾„ï¼ˆä½¿workspaceè·¯å¾„å¯ç”¨ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨scriptsä¸­çš„operator
        modified_code = workflow_code.replace(
            f"import workspace.{problem_type}.workflows.template.operator as operator",
            "# operator already imported"
        )

        # ä¿®å¤å¸¸è§typoï¼ˆRLæ¨¡å‹å¯èƒ½äº§ç”Ÿçš„é”™è¯¯ï¼‰
        modified_code = modified_code.replace("async_lll", "async_llm")
        modified_code = modified_code.replace("create_lll_instance", "create_llm_instance")

        try:
            # æ‰§è¡Œä»£ç åˆ›å»ºç±»
            exec(modified_code, namespace)

            # è¿”å›Workflowç±»
            if "Workflow" not in namespace:
                raise ValueError("No Workflow class found in generated code")

            return namespace["Workflow"]

        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆçš„å·¥ä½œæµä»£ç æœ‰é”™è¯¯: {e}")
            print(f"  ä½¿ç”¨é»˜è®¤fallbackå·¥ä½œæµ")

            # ä½¿ç”¨ç®€å•çš„é»˜è®¤å·¥ä½œæµä½œä¸ºfallback
            return self._get_fallback_workflow_class(problem_type)

    def _get_llm_config(self):
        """è·å–LLMé…ç½®ï¼ˆç¡®ä¿è¿”å›æ­£ç¡®ç±»å‹ï¼‰"""
        from scripts.async_llm import LLMsConfig, LLMConfig

        try:
            if self.llm_configs:
                result = self.llm_configs.get(self.llm_model_name)
            else:
                # å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®
                result = LLMsConfig.default().get(self.llm_model_name)

            # ç±»å‹éªŒè¯ï¼ˆå…³é”®ï¼ï¼‰
            if isinstance(result, LLMConfig):
                return result
            elif isinstance(result, dict):
                # å¦‚æœæ„å¤–è¿”å›äº† dictï¼Œè½¬æ¢ä¸º LLMConfig
                print(f"âš ï¸  è­¦å‘Šï¼šget() è¿”å›äº† dictï¼Œæ­£åœ¨è½¬æ¢ä¸º LLMConfig")
                return LLMConfig(result)
            elif isinstance(result, str):
                return result
            else:
                print(f"âš ï¸  æœªçŸ¥ç±»å‹: {type(result)}ï¼Œé™çº§ä¸ºå­—ç¬¦ä¸²")
                return self.llm_model_name

        except Exception as e:
            print(f"âš ï¸  è·å–LLMé…ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›å­—ç¬¦ä¸²æ¨¡å‹åï¼Œè®© create_llm_instance è‡ªåŠ¨å¤„ç†
            print(f"  é™çº§ä¸ºå­—ç¬¦ä¸²æ¨¡å¼: {self.llm_model_name}")
            return self.llm_model_name

    def _get_fallback_workflow_class(self, problem_type: str):
        """è¿”å›ä¸€ä¸ªç®€å•çš„é»˜è®¤å·¥ä½œæµç±»ï¼ˆç”¨äºç”Ÿæˆå¤±è´¥æ—¶ï¼‰"""

        class FallbackWorkflow:
            def __init__(self, name: str, llm_config, dataset):
                self.name = name
                self.dataset = dataset
                self.llm = create_llm_instance(llm_config)
                self.custom = operator_module.Custom(self.llm)

            async def __call__(self, problem: str, *args, **kwargs):
                """ç®€å•çš„å•æ­¥æ±‚è§£"""
                try:
                    result = await self.custom(
                        input=problem,
                        instruction="Solve this problem step by step and provide the final answer."
                    )
                    # å®‰å…¨åœ°è·å–cost
                    usage = self.llm.get_usage_summary()
                    if isinstance(usage, dict) and "total_cost" in usage:
                        cost = usage["total_cost"]
                    else:
                        cost = 0.0

                    return result['response'], cost
                except Exception as e:
                    print(f"Fallback workflow error: {e}")
                    import traceback
                    traceback.print_exc()
                    return None, 0.0

        return FallbackWorkflow


async def test_executor():
    """æµ‹è¯•AFlowæ‰§è¡Œå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•AFlowæ‰§è¡Œå™¨")
    print("=" * 60)

    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = AFlowExecutor(
        llm_config_path="config/aflow_llm.yaml",
        llm_model_name="gpt-4o-mini",
        timeout=60
    )

    # æµ‹è¯•å·¥ä½œæµä»£ç ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
    test_workflow_code = """
import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        solution = await self.custom(input=problem, instruction="Solve this problem step by step and provide the final answer.")
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

    # æµ‹è¯•é—®é¢˜
    test_problem = "What is 15 + 27?"

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    # æ‰§è¡Œå·¥ä½œæµ
    answer, cost, metadata = await executor.execute_workflow(
        workflow_code=test_workflow_code,
        problem=test_problem,
        problem_type="math"
    )

    print(f"\nâœ… æ‰§è¡Œç»“æœ:")
    print(f"  æˆåŠŸ: {metadata['success']}")
    print(f"  ç­”æ¡ˆ: {answer}")
    print(f"  æˆæœ¬: ${cost:.6f}")
    print(f"  æ—¶é—´: {metadata['execution_time']:.2f}ç§’")


if __name__ == "__main__":
    asyncio.run(test_executor())
