# GRPO è®­ç»ƒå®Œæ•´æµç¨‹è¯¦è§£

**æ–‡æ¡£æ—¶é—´**: 2025-11-16
**è®­ç»ƒçŠ¶æ€**: âœ… æ­£å¸¸è¿è¡Œ (PID 2255398)

---

## ğŸ¯ æ•´ä½“æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GRPO è®­ç»ƒå¾ªç¯ä¸»æµç¨‹                           â”‚
â”‚                                                                   â”‚
â”‚  1. æ•°æ®åŠ è½½     2. Qwenç”Ÿæˆ    3. AFlowæ‰§è¡Œ    4. è¯„ä¼°å¥–åŠ±    5. ç­–ç•¥æ›´æ–° â”‚
â”‚     â¬‡ï¸              â¬‡ï¸              â¬‡ï¸              â¬‡ï¸              â¬‡ï¸      â”‚
â”‚  MATHæ•°æ®é›†  â†’  ç”Ÿæˆä»£ç   â†’  è°ƒç”¨GPTæ‰§è¡Œ  â†’  è®¡ç®—reward  â†’  æ›´æ–°Qwen   â”‚
â”‚  (500é¢˜)      (7Bæœ¬åœ°)     (4o-mini API)   (ç»„ç›¸å¯¹ä¼˜åŠ¿)   (LoRAå¾®è°ƒ)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‚¹**:
- **Qwen æ¨¡å‹**: æœ¬åœ°è¿è¡Œåœ¨ GPU 2-3ï¼Œç”Ÿæˆ Python Workflow ä»£ç 
- **OpenAI API**: ä»…åœ¨æ‰§è¡Œç”Ÿæˆçš„ Workflow æ—¶è°ƒç”¨ï¼Œç”¨äºæ±‚è§£é—®é¢˜
- **è¯„ä¼°æ–¹æ³•**: æ­£ç¡®æ€§ (70%) + æ•ˆç‡ (20%) + ç®€æ´æ€§ (10%)
- **è®­ç»ƒæ–¹æ³•**: GRPOï¼ˆç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œæ¯æ­¥ 4 ä¸ªå€™é€‰ï¼Œé€‰ä¼˜æ›´æ–°

---

## ğŸ“Š å®Œæ•´æ•°æ®æµè¯¦è§£

### Phase 1: æ•°æ®é‡‡æ ·
```python
# train.py:206-214
batch = dataset.sample(batch_size=4)  # é‡‡æ · 4 ä¸ªé—®é¢˜
# ç¤ºä¾‹é—®é¢˜: "If a = 3 and b = 4, what is a^2 + b^2?"
```

**è¾“å‡º**:
```json
{
  "problem": "If a = 3 and b = 4, what is a^2 + b^2?",
  "answer": "25",          # æ ‡å‡†ç­”æ¡ˆï¼ˆç”¨äºè¯„ä¼°ï¼‰
  "type": "algebra",
  "difficulty": "level_1"
}
```

---

### Phase 2: Qwen ç”Ÿæˆ Workflow ä»£ç 

#### 2.1 æ„å»º Prompt
```python
# src/rl_workflow_generator.py:113-154
prompt = f"""Generate a Python Workflow class. Follow the exact template and API signatures.

CRITICAL: Only use operators listed below with their EXACT parameters!

Available Operators:

1. Custom(llm) - Most flexible, for any custom task
   Call: await self.custom(input=str, instruction=str)
   Returns: {'response': str}

2. AnswerGenerate(llm) - Step-by-step reasoning
   Call: await self.answer_generate(input=str)  â† NO instruction parameter!
   Returns: {'thought': str, 'answer': str}

3. Programmer(llm) - Auto-generate and execute Python code
   Call: await self.programmer(problem=str, analysis=str)
   Returns: {'code': str, 'output': str}

Template (complete the __call__ method):

import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        # Example: self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        # Solve: {problem}
        # MUST return (solution, cost) tuple
        # Example: return solution['response'], self.llm.get_usage_summary()["total_cost"]
        pass
"""
```

#### 2.2 Qwen æ¨¡å‹æ¨ç†ï¼ˆæœ¬åœ° GPUï¼‰
```python
# src/rl_workflow_generator.py:187-199
inputs = tokenizer(prompt, return_tensors="pt").to("cuda:2")

# å…³é”®å‚æ•°
outputs = model.generate(
    **inputs,
    max_new_tokens=2048,
    temperature=0.1,      # âœ¨ ä½æ¸©åº¦ = æ›´ä¸¥æ ¼éµå¾ªæ¨¡æ¿
    top_p=0.95,
    do_sample=True
)

generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

**Qwen ç”Ÿæˆçš„å®é™…ä»£ç ç¤ºä¾‹**ï¼ˆä»æ—¥å¿—ä¸­æå–ï¼‰:
```python
class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)
        self.answer_generate = operator.AnswerGenerate(self.llm)
        self.programmer = operator.Programmer(self.llm)

    async def __call__(self, problem: str):
        # Step 1: ä½¿ç”¨ AnswerGenerate ç”Ÿæˆæ¨ç†è¿‡ç¨‹
        thought_response = await self.answer_generate(input=problem)
        thought = thought_response['thought']

        # Step 2: ä½¿ç”¨ Programmer ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç 
        programmer_response = await self.programmer(problem=problem, analysis=thought)
        code = programmer_response['code']
        output = programmer_response['output']

        # Step 3: æå–è§£å†³æ–¹æ¡ˆ
        solution = output.strip() if output else "No solution found"

        # Step 4: è¿”å› (solution, cost) å…ƒç»„
        return solution, self.llm.get_usage_summary()["total_cost"]
```

**å…³é”®ç‚¹**:
- âœ… ç”Ÿæˆäº†æ­£ç¡®çš„ `class Workflow:` ç»“æ„
- âœ… ä½¿ç”¨äº†æ­£ç¡®çš„ç®—å­ APIï¼ˆ`answer_generate(input=problem)` æ²¡æœ‰ instructionï¼‰
- âœ… è¿”å›äº† `(solution, cost)` å…ƒç»„
- âš ï¸ å¯èƒ½åŒ…å«ä¸€äº›å†—ä½™ä»£ç ï¼Œä½†è¯­æ³•æ­£ç¡®

---

### Phase 3: AFlow æ‰§è¡Œ Workflow

#### 3.1 åŠ¨æ€åŠ è½½ç”Ÿæˆçš„ä»£ç 
```python
# src/aflow_executor.py:213-242
namespace = {
    "operator": operator_module,
    "create_llm_instance": create_llm_instance,
    "DatasetType": str
}

# æ‰§è¡Œä»£ç åˆ›å»º Workflow ç±»
exec(workflow_code, namespace)
WorkflowClass = namespace["Workflow"]
```

#### 3.2 å®ä¾‹åŒ– Workflow
```python
# src/aflow_executor.py:109-121
llm_config = self._get_llm_config()  # è¿”å› LLMConfig å®ä¾‹
workflow = WorkflowClass(
    name="rl_generated_workflow",
    llm_config=llm_config,         # gpt-4o-mini é…ç½®
    dataset="math"
)
```

**è¿™æ—¶å‘ç”Ÿäº†ä»€ä¹ˆ**:
```python
# åœ¨ Workflow.__init__ ä¸­
self.llm = create_llm_instance(llm_config)
# åˆ›å»ºäº† AsyncLLM å®ä¾‹ï¼Œé…ç½®ä¸º gpt-4o-mini

self.answer_generate = operator.AnswerGenerate(self.llm)
# åˆ›å»ºäº†ç®—å­ï¼Œå†…éƒ¨æŒæœ‰ gpt-4o-mini çš„ AsyncLLM
```

#### 3.3 è°ƒç”¨ Workflow æ±‚è§£é—®é¢˜ï¼ˆè°ƒç”¨ OpenAI APIï¼‰
```python
# src/aflow_executor.py:154-157
result = await asyncio.wait_for(
    workflow(problem),  # è°ƒç”¨ Workflow.__call__
    timeout=300
)
answer, cost = result
```

**Workflow å†…éƒ¨æ‰§è¡Œæµç¨‹**ï¼ˆä¼šè°ƒç”¨ OpenAI APIï¼‰:

**Step 1**: `await self.answer_generate(input=problem)`
```python
# scripts/operators.py: AnswerGenerate
async def __call__(self, input: str):
    # âš ï¸ è¿™é‡Œä¼šè°ƒç”¨ OpenAI APIï¼
    response = await self.llm.call_with_format(
        prompt=f"Solve step by step:\n{input}",
        format_type="answer_generate"
    )
    return {'thought': '...', 'answer': '25'}
```

**å®é™… API è°ƒç”¨**ï¼ˆä»æ—¥å¿—ä¸­çœ‹åˆ°ï¼‰:
```
Token usage: 138 input + 126 output = 264 total
Cost: $0.000096 ($0.000021 for input, $0.000076 for output)
```
â†’ **è¿™æ˜¯è°ƒç”¨ gpt-4o-mini çš„æˆæœ¬**

**Step 2**: `await self.programmer(problem=problem, analysis=thought)`
```python
# scripts/operators.py: Programmer
async def __call__(self, problem: str, analysis: str):
    # âš ï¸ è¿™é‡Œåˆä¼šè°ƒç”¨ OpenAI APIï¼
    code_response = await self.llm.call_with_format(
        prompt=f"Generate Python code to solve:\n{problem}\n\nAnalysis: {analysis}",
        format_type="code"
    )

    # æ‰§è¡Œç”Ÿæˆçš„ä»£ç 
    output = exec(code_response['code'])

    return {'code': code_response['code'], 'output': output}
```

**å®é™… API è°ƒç”¨**:
```
Token usage: 383 input + 108 output = 491 total
Cost: $0.000122 ($0.000057 for input, $0.000065 for output)
```

**æ€»æˆæœ¬**: $0.000096 + $0.000122 = **$0.000218** per problem

---

### Phase 4: è¯„ä¼°ä¸å¥–åŠ±è®¡ç®—

#### 4.1 æ­£ç¡®æ€§è¯„ä¼°
```python
# src/reward_calculator.py:46-62
def _evaluate_correctness(self, answer, ground_truth):
    # æ ‡å‡†åŒ–ç­”æ¡ˆï¼ˆç§»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
    pred = self._normalize_answer(answer)
    gt = self._normalize_answer(ground_truth)

    if pred == gt:
        return 1.0  # å®Œå…¨æ­£ç¡®
    elif self._partial_match(pred, gt):
        return 0.5  # éƒ¨åˆ†æ­£ç¡®
    else:
        return 0.0  # é”™è¯¯
```

**ç¤ºä¾‹**:
```python
ground_truth = "25"
answer = "25"  # Workflow è¾“å‡º
correctness = 1.0  # âœ… å®Œå…¨æ­£ç¡®
```

#### 4.2 æ•ˆç‡è¯„ä¼°
```python
# src/reward_calculator.py:64-73
def _evaluate_efficiency(self, cost, execution_time):
    # æ ‡å‡†åŒ–ï¼šç›®æ ‡æˆæœ¬ $0.001ï¼Œç›®æ ‡æ—¶é—´ 10ç§’
    cost_score = max(0, 1 - (cost / 0.001))
    time_score = max(0, 1 - (execution_time / 10))

    return 0.5 * cost_score + 0.5 * time_score
```

**ç¤ºä¾‹**:
```python
cost = $0.000218  # å®é™…æˆæœ¬
cost_score = 1 - (0.000218 / 0.001) = 0.782

execution_time = 2.3ç§’
time_score = 1 - (2.3 / 10) = 0.77

efficiency = 0.5 * 0.782 + 0.5 * 0.77 = 0.776
```

#### 4.3 ç®€æ´æ€§è¯„ä¼°
```python
# src/reward_calculator.py:75-86
def _evaluate_simplicity(self, workflow_code):
    # ç»Ÿè®¡ä½¿ç”¨çš„ç®—å­æ•°é‡
    operators_used = 0
    for op in ['Custom', 'AnswerGenerate', 'Programmer', 'ScEnsemble']:
        if f"operator.{op}" in workflow_code:
            operators_used += 1

    # ç†æƒ³æƒ…å†µï¼š1-2 ä¸ªç®—å­
    if operators_used <= 2:
        return 1.0
    else:
        return max(0, 1 - 0.2 * (operators_used - 2))
```

**ç¤ºä¾‹**:
```python
workflow_code = "... AnswerGenerate ... Programmer ..."
operators_used = 2
simplicity = 1.0  # âœ… å®Œç¾ç®€æ´
```

#### 4.4 ç»¼åˆå¥–åŠ±
```python
# src/reward_calculator.py:31-44
reward = (
    0.7 * correctness +   # 0.7 * 1.0 = 0.7
    0.2 * efficiency +    # 0.2 * 0.776 = 0.155
    0.1 * simplicity      # 0.1 * 1.0 = 0.1
)
# Total = 0.955 âœ… é«˜åˆ†ï¼
```

---

### Phase 5: GRPO ç­–ç•¥æ›´æ–°

#### 5.1 ç”Ÿæˆ 4 ä¸ªå€™é€‰ Workflow
```python
# train.py:233-246
outputs = []
for i in range(4):  # æ¯ä¸ªé—®é¢˜ç”Ÿæˆ 4 ä¸ªå€™é€‰
    workflow_code = rl_generator.generate_workflow(
        problem=problem,
        problem_type="math"
    )

    answer, cost, metadata = await executor.execute_workflow(
        workflow_code, problem
    )

    reward = reward_calc.calculate_reward(answer, gt, cost, ...)
    outputs.append((workflow_code, reward))
```

**ç¤ºä¾‹è¾“å‡º**:
```python
[
    ("Workflow_1", reward=0.955),  # AnswerGenerate + Programmer
    ("Workflow_2", reward=0.823),  # Custom only
    ("Workflow_3", reward=0.701),  # ScEnsemble (æ…¢)
    ("Workflow_4", reward=0.645)   # è¯­æ³•é”™è¯¯ï¼Œç”¨fallback
]
```

#### 5.2 è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼ˆGroup-Relative Advantageï¼‰
```python
# train.py:278-284
rewards = [0.955, 0.823, 0.701, 0.645]
mean_reward = np.mean(rewards)  # 0.781
std_reward = np.std(rewards)    # 0.131

advantages = [(r - mean_reward) / (std_reward + 1e-8) for r in rewards]
# [1.33, 0.32, -0.61, -1.04]
```

**è§£é‡Š**:
- `Workflow_1` (adv=1.33): **å¤§å¹…ä¼˜äºå¹³å‡** â†’ å¢åŠ å…¶æ¦‚ç‡
- `Workflow_2` (adv=0.32): **ç•¥ä¼˜äºå¹³å‡** â†’ å°å¹…å¢åŠ 
- `Workflow_3` (adv=-0.61): **ç•¥å·®äºå¹³å‡** â†’ å°å¹…é™ä½
- `Workflow_4` (adv=-1.04): **è¿œå·®äºå¹³å‡** â†’ å¤§å¹…é™ä½

#### 5.3 è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
```python
# train.py:290-306
loss = 0
for i, (workflow_code, advantage) in enumerate(zip(outputs, advantages)):
    # é‡æ–°è®¡ç®— log_prob
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model(**inputs, labels=tokenizer(workflow_code, return_tensors="pt").input_ids)

    log_prob = -outputs.loss  # è´Ÿå¯¹æ•°ä¼¼ç„¶

    # GRPO æŸå¤±ï¼š-log_prob * advantage
    loss += -log_prob * advantage

loss = loss / 4  # å¹³å‡
```

**æ•°å­¦è§£é‡Š**:
```
L = -Î£ log Ï€(a|s) * A(s,a)

å¯¹äº Workflow_1 (A=1.33):
  å¢åŠ  log Ï€(Workflow_1|problem) â†’ æ›´å®¹æ˜“ç”Ÿæˆç±»ä¼¼ä»£ç 

å¯¹äº Workflow_4 (A=-1.04):
  å‡å°‘ log Ï€(Workflow_4|problem) â†’ æ›´éš¾ç”Ÿæˆç±»ä¼¼ä»£ç 
```

#### 5.4 åå‘ä¼ æ’­æ›´æ–° LoRA å‚æ•°
```python
# train.py:310-315
optimizer.zero_grad()
loss.backward()  # è®¡ç®—æ¢¯åº¦
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
optimizer.step()  # æ›´æ–°å‚æ•°
```

**æ›´æ–°çš„å‚æ•°**:
- LoRA å‚æ•°: 20,000,000 ä¸ªï¼ˆä»… 0.26% çš„æ€»å‚æ•°ï¼‰
- åŸºåº§æ¨¡å‹: å†»ç»“ï¼Œä¸æ›´æ–°

**æ•ˆæœ**:
- Qwen å­¦ä¹ åˆ°ï¼š**AnswerGenerate + Programmer** æ˜¯é«˜å¥–åŠ±çš„æ¨¡å¼
- é€æ¸å‡å°‘ç”Ÿæˆ **ScEnsemble**ï¼ˆå¤ªæ…¢ï¼‰å’Œé”™è¯¯ä»£ç 

---

## ğŸ”„ è®­ç»ƒå¾ªç¯å®Œæ•´æ—¶åºå›¾

```
æ—¶åˆ» T=0 (Step 1 å¼€å§‹)
â”œâ”€ [00:00] é‡‡æ · 4 ä¸ªé—®é¢˜
â”‚
â”œâ”€ [00:05] Qwen ç”Ÿæˆ 4Ã—4=16 ä¸ª Workflowï¼ˆæœ¬åœ° GPUï¼‰
â”‚   â”œâ”€ Problem 1 â†’ [Workflow_1a, 1b, 1c, 1d]
â”‚   â”œâ”€ Problem 2 â†’ [Workflow_2a, 2b, 2c, 2d]
â”‚   â”œâ”€ Problem 3 â†’ [Workflow_3a, 3b, 3c, 3d]
â”‚   â””â”€ Problem 4 â†’ [Workflow_4a, 4b, 4c, 4d]
â”‚
â”œâ”€ [00:10] æ‰§è¡Œ 16 ä¸ª Workflowï¼ˆè°ƒç”¨ OpenAI APIï¼‰
â”‚   â”œâ”€ Workflow_1a(problem_1) â†’ API call â†’ answer, cost
â”‚   â”œâ”€ Workflow_1b(problem_1) â†’ API call â†’ answer, cost
â”‚   â””â”€ ... (16 æ¬¡ API è°ƒç”¨ï¼Œå¹¶å‘æ‰§è¡Œ)
â”‚
â”œâ”€ [00:35] è®¡ç®— 16 ä¸ªå¥–åŠ±
â”‚   â””â”€ æ¯ä¸ª Workflow â†’ [correctness, efficiency, simplicity] â†’ reward
â”‚
â”œâ”€ [00:40] è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼ˆ4 ç»„ï¼Œæ¯ç»„ 4 ä¸ªï¼‰
â”‚   â”œâ”€ Problem 1: advantages = [1.2, 0.5, -0.8, -0.9]
â”‚   â””â”€ Problem 2-4: ç±»ä¼¼
â”‚
â”œâ”€ [00:45] è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
â”‚   â””â”€ loss = -Î£ log_prob * advantage
â”‚
â”œâ”€ [00:50] æ›´æ–° LoRA å‚æ•°
â”‚   â””â”€ optimizer.step()
â”‚
â””â”€ [00:55] ä¿å­˜æ£€æŸ¥ç‚¹ â†’ Step 2 å¼€å§‹

æ€»è€—æ—¶ï¼šçº¦ 55 ç§’/step
æ€» API è°ƒç”¨ï¼š16 æ¬¡/stepï¼ˆbatch_size=4, candidates=4ï¼‰
æ€» API æˆæœ¬ï¼šçº¦ $0.0035/stepï¼ˆ16 Ã— $0.00022ï¼‰
```

---

## ğŸ¤– Qwen vs OpenAI API èŒè´£åˆ’åˆ†

### Qwen2.5-7B-Instruct (æœ¬åœ° GPU 2-3)
**èŒè´£**: ç”Ÿæˆ Workflow ä»£ç ï¼ˆç­–ç•¥ç½‘ç»œï¼‰

**è¾“å…¥**:
```
Prompt: "Generate a Workflow to solve: If a=3, b=4, what is a^2+b^2?"
```

**è¾“å‡º**:
```python
class Workflow:
    async def __call__(self, problem: str):
        result = await self.answer_generate(input=problem)
        return result['answer'], self.llm.get_usage_summary()["total_cost"]
```

**è¿è¡Œé¢‘ç‡**:
- æ¯ä¸ªé—®é¢˜ç”Ÿæˆ 4 æ¬¡ï¼ˆæ¢ç´¢ä¸åŒç­–ç•¥ï¼‰
- æ¯æ­¥ 4 ä¸ªé—®é¢˜ â†’ 16 æ¬¡ç”Ÿæˆ
- 500 æ­¥ â†’ 8000 æ¬¡ç”Ÿæˆ

**æˆæœ¬**: å…è´¹ï¼ˆæœ¬åœ°è¿è¡Œï¼‰

---

### OpenAI gpt-4o-mini (API)
**èŒè´£**: æ‰§è¡Œ Workflow ä¸­çš„ç®—å­ï¼ˆæ±‚è§£å®é™…é—®é¢˜ï¼‰

**è°ƒç”¨åœºæ™¯**:
1. **Custom ç®—å­**: è‡ªå®šä¹‰æŒ‡ä»¤
   ```python
   await self.custom(input="What is 3^2 + 4^2?", instruction="Solve step by step")
   # API è°ƒç”¨: gpt-4o-mini
   ```

2. **AnswerGenerate ç®—å­**: æ¨ç†æ±‚è§£
   ```python
   await self.answer_generate(input="What is 3^2 + 4^2?")
   # API è°ƒç”¨: gpt-4o-mini
   ```

3. **Programmer ç®—å­**: ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç 
   ```python
   await self.programmer(problem="What is 3^2 + 4^2?", analysis="...")
   # API è°ƒç”¨: gpt-4o-miniï¼ˆç”Ÿæˆä»£ç ï¼‰
   ```

**è¿è¡Œé¢‘ç‡**:
- æ¯ä¸ª Workflow è°ƒç”¨ 1-3 æ¬¡ï¼ˆå–å†³äºç®—å­æ•°é‡ï¼‰
- å¹³å‡ 2 æ¬¡/Workflow
- æ¯æ­¥ 16 ä¸ª Workflow Ã— 2 = 32 æ¬¡ API è°ƒç”¨
- 500 æ­¥ â†’ 16,000 æ¬¡ API è°ƒç”¨

**æˆæœ¬**:
- å•æ¬¡: $0.0001-0.0003
- æ¯æ­¥: $0.005-0.01
- æ€»è®¡ï¼ˆ500æ­¥ï¼‰: **$2.5-5.0**

---

## ğŸ“ˆ è¯„ä¼°æ–¹æ³•è¯¦è§£

### 1. æ­£ç¡®æ€§è¯„ä¼°ï¼ˆ70% æƒé‡ï¼‰

#### æ–¹æ³• 1: ç²¾ç¡®åŒ¹é…
```python
def exact_match(pred, gt):
    return 1.0 if normalize(pred) == normalize(gt) else 0.0

# ç¤ºä¾‹
exact_match("25", "25") â†’ 1.0
exact_match("25.0", "25") â†’ 1.0 (æ ‡å‡†åŒ–åç›¸åŒ)
exact_match("24", "25") â†’ 0.0
```

#### æ–¹æ³• 2: æ•°å€¼å®¹å·®åŒ¹é…ï¼ˆæ•°å­¦é—®é¢˜ï¼‰
```python
def numerical_match(pred, gt, tolerance=1e-4):
    try:
        pred_num = float(extract_number(pred))
        gt_num = float(extract_number(gt))
        return 1.0 if abs(pred_num - gt_num) < tolerance else 0.0
    except:
        return exact_match(pred, gt)

# ç¤ºä¾‹
numerical_match("25.0001", "25") â†’ 1.0
numerical_match("25.1", "25") â†’ 0.0
```

#### æ–¹æ³• 3: éƒ¨åˆ†åŒ¹é…ï¼ˆå¤æ‚ç­”æ¡ˆï¼‰
```python
def partial_match(pred, gt):
    # æ£€æŸ¥å…³é”®å­—æ˜¯å¦å­˜åœ¨
    pred_tokens = set(tokenize(pred))
    gt_tokens = set(tokenize(gt))

    overlap = len(pred_tokens & gt_tokens) / len(gt_tokens)

    if overlap > 0.8:
        return 1.0
    elif overlap > 0.5:
        return 0.5
    else:
        return 0.0
```

---

### 2. æ•ˆç‡è¯„ä¼°ï¼ˆ20% æƒé‡ï¼‰

#### å…¬å¼
```python
efficiency = 0.5 * cost_score + 0.5 * time_score

cost_score = max(0, 1 - cost / cost_target)
time_score = max(0, 1 - time / time_target)
```

#### ç›®æ ‡å€¼
- `cost_target` = $0.001ï¼ˆå•ä¸ªé—®é¢˜ï¼‰
- `time_target` = 10ç§’

#### ç¤ºä¾‹
```python
# Workflow A: AnswerGenerate only
cost = $0.0001, time = 1.5ç§’
cost_score = 1 - 0.0001/0.001 = 0.9
time_score = 1 - 1.5/10 = 0.85
efficiency = 0.5 * 0.9 + 0.5 * 0.85 = 0.875

# Workflow B: AnswerGenerate + Programmer + ScEnsemble
cost = $0.0015, time = 15ç§’
cost_score = 1 - 0.0015/0.001 = 0 (è¶…é¢„ç®—)
time_score = 1 - 15/10 = 0 (è¶…æ—¶)
efficiency = 0.0
```

**å¥–åŠ±ä¿¡å·**: Qwen å­¦ä¹ é¿å…ä½¿ç”¨æ˜‚è´µçš„ç®—å­ç»„åˆ

---

### 3. ç®€æ´æ€§è¯„ä¼°ï¼ˆ10% æƒé‡ï¼‰

#### ç®—å­è®¡æ•°
```python
def count_operators(code):
    count = 0
    for op in ['Custom', 'AnswerGenerate', 'Programmer', 'ScEnsemble', 'Review', 'Revise']:
        if f"operator.{op}" in code:
            count += 1
    return count

def simplicity_score(count):
    if count <= 2:
        return 1.0
    elif count == 3:
        return 0.8
    elif count == 4:
        return 0.6
    else:
        return max(0, 0.6 - 0.2 * (count - 4))
```

#### ç¤ºä¾‹
```python
# Workflow A: åªç”¨ Custom
count = 1 â†’ simplicity = 1.0

# Workflow B: AnswerGenerate + Programmer
count = 2 â†’ simplicity = 1.0

# Workflow C: AnswerGenerate + Programmer + Review + Revise
count = 4 â†’ simplicity = 0.6
```

**å¥–åŠ±ä¿¡å·**: Qwen å­¦ä¹ ç”¨æœ€å°‘çš„ç®—å­å®Œæˆä»»åŠ¡

---

## ğŸ’¡ å¥–åŠ±å‡½æ•°è®¾è®¡ç†å¿µ

### ç»„åˆç­–ç•¥
```python
reward = 0.7 * correctness + 0.2 * efficiency + 0.1 * simplicity
```

### æƒé‡è®¾è®¡åŸå› 

#### 70% æ­£ç¡®æ€§
- **ç†ç”±**: é”™è¯¯ç­”æ¡ˆæ— ä»·å€¼ï¼Œå¿…é¡»ä¼˜å…ˆä¿è¯æ­£ç¡®
- **æ•ˆæœ**: å³ä½¿æ•ˆç‡ä½ï¼Œåªè¦æ­£ç¡®ä¹Ÿèƒ½å¾— 0.7 åˆ†
- **è®­ç»ƒç›®æ ‡**: Qwen é¦–å…ˆå­¦ä¼šç”Ÿæˆèƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆçš„ Workflow

#### 20% æ•ˆç‡
- **ç†ç”±**: åœ¨ä¿è¯æ­£ç¡®çš„å‰æä¸‹ï¼Œä¼˜åŒ–æˆæœ¬å’Œé€Ÿåº¦
- **æ•ˆæœ**: æ­£ç¡®ä¸”é«˜æ•ˆçš„ Workflow å¾—åˆ† 0.9+
- **è®­ç»ƒç›®æ ‡**: Qwen å­¦ä¼šé¿å…ä¸å¿…è¦çš„å¤šæ­¥è°ƒç”¨

#### 10% ç®€æ´æ€§
- **ç†ç”±**: ç®€æ´ä»£ç æ›´æ˜“ç»´æŠ¤ï¼Œä¹Ÿæš—ç¤ºæ›´å¥½çš„ç­–ç•¥
- **æ•ˆæœ**: ç®€å•ç›´æ¥çš„è§£æ³•æ¯”å¤æ‚çš„ç•¥ä¼˜
- **è®­ç»ƒç›®æ ‡**: Qwen å­¦ä¼šç”¨æœ€ç®€å•çš„æ–¹å¼è§£å†³é—®é¢˜

### è®¾è®¡æƒè¡¡

#### åœºæ™¯ 1: å¤æ‚ä½†å‡†ç¡® vs ç®€å•ä½†é”™è¯¯
```python
Workflow A: AnswerGenerate + Programmer + Review (å¤æ‚)
  correctness = 1.0, efficiency = 0.7, simplicity = 0.8
  reward = 0.7Ã—1.0 + 0.2Ã—0.7 + 0.1Ã—0.8 = 0.92

Workflow B: Custom only (ç®€å•)
  correctness = 0.6, efficiency = 0.9, simplicity = 1.0
  reward = 0.7Ã—0.6 + 0.2Ã—0.9 + 0.1Ã—1.0 = 0.70

âœ… Workflow A è·èƒœï¼ˆæ­£ç¡®æ€§ä¸ºç‹ï¼‰
```

#### åœºæ™¯ 2: ä¸¤è€…éƒ½æ­£ç¡®ï¼Œä½†æ•ˆç‡ä¸åŒ
```python
Workflow A: AnswerGenerate only
  correctness = 1.0, efficiency = 0.9, simplicity = 1.0
  reward = 0.7Ã—1.0 + 0.2Ã—0.9 + 0.1Ã—1.0 = 0.98

Workflow B: AnswerGenerate + Programmer + ScEnsemble
  correctness = 1.0, efficiency = 0.3, simplicity = 0.6
  reward = 0.7Ã—1.0 + 0.2Ã—0.3 + 0.1Ã—0.6 = 0.82

âœ… Workflow A è·èƒœï¼ˆç®€å•é«˜æ•ˆï¼‰
```

---

## ğŸ”§ GRPO ç®—æ³•æ ¸å¿ƒ

### ä¸ºä»€ä¹ˆç”¨ GRPO è€Œä¸æ˜¯ PPOï¼Ÿ

#### PPO é—®é¢˜
- éœ€è¦å¤§é‡æ ·æœ¬ï¼ˆmillionsï¼‰
- éœ€è¦ Value Networkï¼ˆé¢å¤– 7B å‚æ•°ï¼‰
- è®­ç»ƒä¸ç¨³å®šï¼ˆKL æ•£åº¦éš¾æ§åˆ¶ï¼‰

#### GRPO ä¼˜åŠ¿
- åªéœ€å¯¹æ¯”ç»„å†…ç›¸å¯¹ä¼˜åŠ£ï¼ˆ4 ä¸ªå€™é€‰å³å¯ï¼‰
- ä¸éœ€è¦ Value Networkï¼ˆèŠ‚çœå†…å­˜ï¼‰
- ç¨³å®šæ€§æ›´å¥½ï¼ˆç»„å½’ä¸€åŒ–ï¼‰

### GRPO æ•°å­¦åŸç†

#### æ ‡å‡† RL ç›®æ ‡
```
maximize E[R(s,a)]  # æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
```

#### PPO ç›®æ ‡
```
L = E[min(
    Ï€_new(a|s)/Ï€_old(a|s) * A(s,a),
    clip(Ï€_new(a|s)/Ï€_old(a|s), 1-Îµ, 1+Îµ) * A(s,a)
)]
```

#### GRPO ç›®æ ‡
```
å¯¹äºæ¯ç»„ {a1, a2, a3, a4}:
  mean_R = mean([R(s,a1), R(s,a2), R(s,a3), R(s,a4)])
  std_R = std([R(s,a1), R(s,a2), R(s,a3), R(s,a4)])

  A(s,ai) = (R(s,ai) - mean_R) / (std_R + 1e-8)

L = E[log Ï€(a|s) * A(s,a)]
```

**å…³é”®åŒºåˆ«**: ä¼˜åŠ¿ A åŸºäº**ç»„å†…ç›¸å¯¹è¡¨ç°**ï¼Œè€Œéå…¨å±€ baseline

---

## ğŸ“Š è®­ç»ƒç›‘æ§æŒ‡æ ‡

### ä¸»è¦æŒ‡æ ‡

```python
metrics = {
    "avg_reward": 0.82,           # å¹³å‡å¥–åŠ±
    "avg_correctness": 0.75,      # å¹³å‡æ­£ç¡®ç‡
    "avg_cost": 0.00025,          # å¹³å‡APIæˆæœ¬
    "avg_execution_time": 3.2,    # å¹³å‡æ‰§è¡Œæ—¶é—´
    "fallback_rate": 0.15,        # Fallbackä½¿ç”¨ç‡
    "valid_generation_rate": 0.85 # æœ‰æ•ˆç”Ÿæˆç‡
}
```

### æœŸæœ›è¶‹åŠ¿

#### æˆåŠŸè®­ç»ƒçš„ä¿¡å·
- âœ… `avg_reward` ä» 0.6 â†’ 0.9+
- âœ… `avg_correctness` ä» 0.5 â†’ 0.95+
- âœ… `fallback_rate` ä» 0.5 â†’ 0.05
- âœ… `valid_generation_rate` ä» 0.6 â†’ 0.98+
- âœ… `avg_cost` ä¿æŒç¨³å®šæˆ–é™ä½

#### å¤±è´¥è®­ç»ƒçš„ä¿¡å·
- âŒ `avg_reward` éœ‡è¡ä¸æ”¶æ•›
- âŒ `fallback_rate` å±…é«˜ä¸ä¸‹ (>0.3)
- âŒ `avg_cost` æŒç»­ä¸Šå‡
- âŒ Loss çˆ†ç‚¸æˆ– NaN

---

## ğŸ¯ å½“å‰è®­ç»ƒçŠ¶æ€

**è¿›ç¨‹**: PID 2255398
**GPU**: 2-3 (CUDA_VISIBLE_DEVICES)
**Step**: 1/500
**æ¨¡å‹**: Qwen2.5-7B-Instruct + LoRA
**æ¸©åº¦**: 0.1ï¼ˆä¸¥æ ¼æ¨¡æ¿éµå¾ªï¼‰

**æœ€æ–°ç”Ÿæˆè´¨é‡**:
- âœ… ç”Ÿæˆæ­£ç¡®çš„ `class Workflow:` ç»“æ„
- âœ… ä½¿ç”¨æ­£ç¡®çš„ç®—å­ API
- âœ… è¿”å›æ­£ç¡®çš„ `(solution, cost)` å…ƒç»„
- âš ï¸ å¶å°”æ·»åŠ ä¸å¿…è¦çš„è¾…åŠ©æ–¹æ³•ï¼ˆå°†é€šè¿‡è®­ç»ƒæ”¹è¿›ï¼‰

**API è°ƒç”¨æƒ…å†µ**:
- âœ… æ¯ä¸ª Workflow æ‰§è¡Œæ—¶è°ƒç”¨ gpt-4o-mini
- âœ… å…¸å‹æˆæœ¬: $0.0002-0.0003/é—®é¢˜
- âœ… Qwen æœ¬èº«ä¸è°ƒç”¨ APIï¼ˆå®Œå…¨æœ¬åœ°è¿è¡Œï¼‰

---

## ğŸ“‹ æ€»ç»“

### Qwen åœ¨åšä»€ä¹ˆï¼Ÿ
**ç”Ÿæˆ Python ä»£ç **ï¼ˆWorkflow ç±»ï¼‰ï¼Œç”¨äºç»„åˆ AFlow ç®—å­æ¥è§£å†³é—®é¢˜ã€‚

### OpenAI API åœ¨åšä»€ä¹ˆï¼Ÿ
**æ‰§è¡Œç®—å­ä¸­çš„å®é™…æ¨ç†**ï¼ˆæ±‚è§£æ•°å­¦é¢˜ã€ç”Ÿæˆä»£ç ç­‰ï¼‰ã€‚

### è¯„ä¼°æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ
**ä¸‰ç»´è¯„åˆ†**: æ­£ç¡®æ€§ï¼ˆ70%ï¼‰+ æ•ˆç‡ï¼ˆ20%ï¼‰+ ç®€æ´æ€§ï¼ˆ10%ï¼‰ã€‚

### å¥–åŠ±å‡½æ•°å¦‚ä½•è®¾è®¡ï¼Ÿ
**ç»„ç›¸å¯¹ä¼˜åŠ¿**: åŒä¸€é—®é¢˜çš„ 4 ä¸ªå€™é€‰ç›¸äº’æ¯”è¾ƒï¼Œä¼˜è€…å¢å¼ºã€åŠ£è€…æŠ‘åˆ¶ã€‚

### å®Œæ•´æµç¨‹æ€»ç»“
```
1. é‡‡æ ·é—®é¢˜ â†’ 2. Qwenç”Ÿæˆä»£ç (æœ¬åœ°) â†’ 3. æ‰§è¡Œä»£ç (è°ƒç”¨API) â†’
4. è¯„ä¼°å¥–åŠ± â†’ 5. è®¡ç®—ä¼˜åŠ¿ â†’ 6. æ›´æ–°Qwenå‚æ•° â†’ é‡å¤
```

---

**æ–‡æ¡£å®Œæˆ** âœ…
**ä¸‹ä¸€æ­¥**: æŒç»­ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œè§‚å¯Ÿ `avg_reward` å’Œ `fallback_rate` è¶‹åŠ¿
