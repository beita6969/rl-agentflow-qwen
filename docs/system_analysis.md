# AFlow + GRPO é›†æˆç³»ç»Ÿåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-11-16
**è®­ç»ƒçŠ¶æ€**: Step 2/500 è¿è¡Œä¸­
**æ ¸å¿ƒé—®é¢˜**: Qwen2.5-7Bæœªèƒ½ç”Ÿæˆæ­£ç¡®çš„Workflowç±»æ ¼å¼

---

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### 1.1 æ ¸å¿ƒç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GRPO Training Loop                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Data Manager â”‚  â”‚ RL Generator â”‚  â”‚  AFlow       â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  Executor    â”‚      â”‚
â”‚  â”‚ é‡‡æ ·é—®é¢˜     â”‚â”€>â”‚ Qwen2.5-7B  â”‚â”€>â”‚  æ‰§è¡Œå·¥ä½œæµ  â”‚      â”‚
â”‚  â”‚ (math/code/  â”‚  â”‚ + LoRA      â”‚  â”‚  (gpt-4o-    â”‚      â”‚
â”‚  â”‚  qa)         â”‚  â”‚              â”‚  â”‚   mini)      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                 â”‚                   â”‚             â”‚
â”‚         v                 v                   v             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚         Reward Computer                          â”‚      â”‚
â”‚  â”‚  - æ­£ç¡®æ€§ (70%)                                  â”‚      â”‚
â”‚  â”‚  - æ•ˆç‡ (20%)                                    â”‚      â”‚
â”‚  â”‚  - ç®€æ´æ€§ (10%)                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                  â”‚
â”‚                          v                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚    Policy Update (GRPO Algorithm)                â”‚      â”‚
â”‚  â”‚  - ç»„å†…ä¼˜åŠ¿å½’ä¸€åŒ–                                â”‚      â”‚
â”‚  â”‚  - PPOè£å‰ªæŸå¤±                                   â”‚      â”‚
â”‚  â”‚  - LoRAæƒé‡æ›´æ–°                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ–‡ä»¶ç»“æ„

```
integrated_aflow_roll/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ grpo_trainer.py          # GRPOè®­ç»ƒä¸»å¾ªç¯
â”‚   â”œâ”€â”€ rl_workflow_generator.py # Qwen2.5-7Bç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ aflow_executor.py        # AFlowå·¥ä½œæµæ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ reward_computer.py       # å¥–åŠ±è®¡ç®—
â”‚   â”œâ”€â”€ data_manager.py          # æ•°æ®é‡‡æ ·
â”‚   â””â”€â”€ gpu_manager.py           # GPUèµ„æºç®¡ç†
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ training.yaml            # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ aflow_llm.yaml           # AFlow LLMé…ç½®
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/mixed_dataset.jsonl (80æ ·æœ¬)
â”‚   â”œâ”€â”€ val/mixed_dataset.jsonl   (10æ ·æœ¬)
â”‚   â””â”€â”€ test/mixed_dataset.jsonl  (10æ ·æœ¬)
â””â”€â”€ logs/
    â””â”€â”€ training_output.log      # è®­ç»ƒæ—¥å¿—
```

---

## 2. å®Œæ•´è®­ç»ƒæµç¨‹è¯¦è§£

### 2.1 åˆå§‹åŒ–é˜¶æ®µ

1. **GPUç®¡ç†å™¨åˆå§‹åŒ–**
   - éªŒè¯ç‰©ç†GPU 2-3å¯ç”¨
   - ä¿æŠ¤è¿›ç¨‹PID 3819483
   - è®¾ç½® `CUDA_VISIBLE_DEVICES=2,3`

2. **æ•°æ®åŠ è½½**
   ```
   è®­ç»ƒé›†: 80æ ·æœ¬ (math: 30, code: 26, qa: 24)
   éªŒè¯é›†: 10æ ·æœ¬ (math: 5, code: 2, qa: 3)
   æµ‹è¯•é›†: 10æ ·æœ¬ (math: 5, code: 2, qa: 3)

   é‡‡æ ·æ¯”ä¾‹: math 40%, code 30%, qa 30%
   ```

3. **æ¨¡å‹åŠ è½½**
   - **åŸºåº§æ¨¡å‹**: Qwen2.5-7B-Instruct (7.6Bå‚æ•°)
   - **LoRAé€‚é…å™¨**: rank=32, alpha=32
   - **å¯è®­ç»ƒå‚æ•°**: 20.2M (0.26%)
   - **åŠ è½½æ—¶é—´**: ~40ç§’ (ä½¿ç”¨æœ¬åœ°æ¨¡å‹)

4. **AFlowç»„ä»¶åˆå§‹åŒ–**
   - LLM: gpt-4o-mini
   - è¶…æ—¶: 180ç§’
   - ç®—å­: Custom, AnswerGenerate, Programmer, etc.

### 2.2 å•æ­¥è®­ç»ƒæµç¨‹ (train_step)

**æ¯ä¸ªStepçš„è¯¦ç»†è¿‡ç¨‹**:

#### Step 1: é‡‡æ ·é—®é¢˜ (batch_size=4)

```python
# grpo_trainer.py:154-161
batch = self.data_manager.sample_batch(
    batch_size=4,  # æ¯æ‰¹4ä¸ªé—®é¢˜
    split="train"
)
# ç¤ºä¾‹è¾“å‡º: {'math': 2, 'code': 1, 'qa': 1}
```

#### Step 2: ä¸ºæ¯ä¸ªé—®é¢˜ç”ŸæˆKä¸ªå·¥ä½œæµ (K=4)

```python
# grpo_trainer.py:172-190
for sample in batch:  # 4ä¸ªé—®é¢˜
    for i in range(4):  # æ¯ä¸ªé—®é¢˜ç”Ÿæˆ4ä¸ªå·¥ä½œæµ
        # 2.1 ç”Ÿæˆå·¥ä½œæµä»£ç 
        result = self.generator.generate_workflow(
            problem=problem,
            problem_type=problem_type,
            temperature=0.1  # æ–°ä¼˜åŒ–: ä»0.7é™ä½
        )
        # é¢„æœŸè¾“å‡º: Workflowç±»çš„Pythonä»£ç 
        # å®é™…è¾“å‡º: def solve() å‡½æ•° âŒ
```

**ç”Ÿæˆè¿‡ç¨‹è¯¦è§£**:

```python
# rl_workflow_generator.py:177-211
def generate_workflow(problem, problem_type, temperature=0.1):
    # 1. æ„å»ºæç¤ºè¯
    prompt = self._build_generation_prompt(problem, problem_type)

    # å½“å‰æç¤ºè¯ (ä¼˜åŒ–å):
    """
    Complete the following Python Workflow class.
    DO NOT write explanations or comments.
    Only generate valid Python code.

    import workspace.math.workflows.template.operator as operator
    from scripts.async_llm import create_llm_instance
    from scripts.evaluator import DatasetType

    class Workflow:
        def __init__(self, name: str, llm_config, dataset: DatasetType):
            self.name = name
            self.dataset = dataset
            self.llm = create_llm_instance(llm_config)
            self.custom = operator.Custom(self.llm)

        async def __call__(self, problem: str):
            # Use operators to solve: {problem}
            solution = await self.custom(
                input=problem,
                instruction="Solve this problem step by step."
            )
            return solution['response'], self.llm.get_usage_summary()["total_cost"]
    """

    # 2. Tokenize + ç”Ÿæˆ
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=4096,
        temperature=0.1,      # ä½æ¸©åº¦ = ä¸¥æ ¼éµå¾ª
        top_p=0.95,
        top_k=50,
        do_sample=True
    )

    # 3. è§£ç 
    generated_text = tokenizer.decode(outputs[0][len(inputs):])

    # 4. è§£æä»£ç 
    workflow_code, is_valid, error = self._parse_workflow_code(generated_text)

    return {"workflow_code": workflow_code, "valid": is_valid, "error": error}
```

#### Step 3: è®¡ç®—æ—§ç­–ç•¥çš„logæ¦‚ç‡

```python
# grpo_trainer.py:194
log_prob = await self._compute_log_prob(problem, workflow_code, problem_type)

# å®ç° (grpo_trainer.py:261-285):
def _compute_log_prob(problem, workflow_code, problem_type):
    with torch.no_grad():
        full_text = prompt + workflow_code
        inputs = tokenizer(full_text, return_tensors="pt").to(device)
        outputs = model(**inputs, labels=inputs["input_ids"])
        log_prob = -outputs.loss  # è´Ÿå¯¹æ•°ä¼¼ç„¶
    return log_prob.detach().cpu()
```

#### Step 4: æ‰§è¡Œå·¥ä½œæµ

```python
# grpo_trainer.py:198-203
answer, cost, metadata = await executor.execute_workflow(
    workflow_code=workflow_code,
    problem=problem,
    problem_type=problem_type,
    entry_point=sample.get('entry_point', '')
)
```

**æ‰§è¡Œæµç¨‹è¯¦è§£**:

```python
# aflow_executor.py:74-196
async def execute_workflow(workflow_code, problem, problem_type):
    try:
        # 1. åŠ¨æ€åˆ›å»ºWorkflowç±»
        workflow_class = self._create_workflow_class(workflow_code, problem_type)

        # 2. å®ä¾‹åŒ–
        workflow = workflow_class(
            name="rl_generated_workflow",
            llm_config=llm_config,
            dataset=problem_type
        )

        # 3. æ‰§è¡Œ (å¸¦è¶…æ—¶180ç§’)
        result = await asyncio.wait_for(
            workflow(problem),
            timeout=180
        )

        # 4. è§£åŒ…ç»“æœ
        answer, cost = result[0], result[1]

    except Exception as e:
        # âš ï¸ å…³é”®: å¦‚æœç”Ÿæˆçš„ä»£ç æœ‰é”™è¯¯ï¼Œä½¿ç”¨fallback
        fallback_class = self._get_fallback_workflow_class(problem_type)
        workflow = fallback_class(...)
        result = await workflow(problem)
        answer, cost = result[0], result[1]

    return answer, cost, metadata
```

**Fallbackå·¥ä½œæµ**:

```python
# aflow_executor.py:251-282
class FallbackWorkflow:
    def __init__(self, name, llm_config, dataset):
        self.llm = create_llm_instance(llm_config)  # gpt-4o-mini
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem):
        result = await self.custom(
            input=problem,
            instruction="Solve this problem step by step and provide the final answer."
        )
        return result['response'], self.llm.get_usage_summary()["total_cost"]
```

#### Step 5: è®¡ç®—å¥–åŠ±

```python
# grpo_trainer.py:206-215
if metadata['success']:
    reward = self.reward_computer.compute_reward(
        problem=problem,
        prediction=answer,
        ground_truth=ground_truth,
        problem_type=problem_type,
        metadata=metadata
    )
else:
    reward = -10.0  # æ‰§è¡Œå¤±è´¥æƒ©ç½š
```

**å¥–åŠ±è®¡ç®—å…¬å¼**:

```python
# reward_computer.py (æ¨æ–­)
def compute_reward(problem, prediction, ground_truth, problem_type, metadata):
    # 1. æ­£ç¡®æ€§ (70%)
    correctness = check_answer(prediction, ground_truth, problem_type)
    # - math: æå–æœ€åæ•°å­—æ¯”è¾ƒ
    # - code: è¿è¡Œæµ‹è¯•ç”¨ä¾‹
    # - qa: è¯­ä¹‰ç›¸ä¼¼åº¦

    # 2. æ•ˆç‡ (20%) - è´Ÿæˆæœ¬
    efficiency = -metadata['cost']

    # 3. ç®€æ´æ€§ (10%) - è´Ÿç®—å­æ•°
    simplicity = -count_operators(workflow_code)

    total_reward = (
        0.7 * correctness +
        0.2 * efficiency +
        0.1 * simplicity
    )

    return total_reward
```

#### Step 6: GRPOç»„å†…ä¼˜åŠ¿å½’ä¸€åŒ–

```python
# grpo_trainer.py:227-229
# å¯¹æ¯ä¸ªé—®é¢˜çš„4ä¸ªå·¥ä½œæµ:
mean_reward = np.mean(group_rewards)  # [r1, r2, r3, r4]
group_advantages = [r - mean_reward for r in group_rewards]
# ç¤ºä¾‹: rewards=[5, 3, 4, 8] -> advantages=[-0, -2, -1, 3]
```

#### Step 7: ç­–ç•¥æ›´æ–° (PPO with GRPO)

```python
# grpo_trainer.py:287-368
async def _update_policy(problems, workflows, old_log_probs, advantages):
    for j in range(len(workflows)):
        # 1. è®¡ç®—æ–°logæ¦‚ç‡ (å¯è®­ç»ƒ)
        new_log_prob = await self._compute_log_prob_trainable(
            problem, workflow, problem_type
        )

        # 2. é‡è¦æ€§é‡‡æ ·æ¯”
        ratio = torch.exp(new_log_prob - old_log_prob)

        # 3. PPOè£å‰ªæŸå¤±
        clip_range = 0.2
        clipped_ratio = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)

        policy_loss = -torch.min(
            ratio * advantage,
            clipped_ratio * advantage
        )

        # 4. KLæ­£åˆ™åŒ– (å¯é€‰)
        kl_loss = 0.001 * (new_log_prob - old_log_prob).pow(2)

        # 5. æ€»æŸå¤±
        loss = policy_loss + kl_loss

    # 6. åå‘ä¼ æ’­
    loss.backward()

    # 7. æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # 8. ä¼˜åŒ–å™¨æ­¥éª¤
    optimizer.step()
    optimizer.zero_grad()
```

### 2.3 è®­ç»ƒè¶…å‚æ•°

```yaml
# config/training.yaml
max_steps: 500
rollout_batch_size: 4              # æ¯æ‰¹4ä¸ªé—®é¢˜
num_return_sequences_in_group: 4  # æ¯ä¸ªé—®é¢˜4ä¸ªå·¥ä½œæµ
ppo_epochs: 1                      # åœ¨çº¿å­¦ä¹ 
clip_range: 0.2
learning_rate: 1e-5
gradient_accumulation_steps: 4

# ç”Ÿæˆé…ç½®
temperature: 0.1                   # ä¼˜åŒ–å (åŸ0.7)
top_p: 0.95
top_k: 50
max_new_tokens: 4096

# å¥–åŠ±æƒé‡
reward_weights:
  correctness: 0.7
  efficiency: 0.2
  simplicity: 0.1
```

---

## 3. æ ¸å¿ƒé—®é¢˜æ·±åº¦åˆ†æ

### 3.1 é—®é¢˜è¡¨ç°

**è§‚å¯Ÿåˆ°çš„ç°è±¡**:

```python
# é¢„æœŸç”Ÿæˆ:
class Workflow:
    def __init__(self, name, llm_config, dataset):
        self.name = name
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem):
        solution = await self.custom(input=problem, ...)
        return solution['response'], cost

# å®é™…ç”Ÿæˆ (æ¥è‡ªæ—¥å¿—):
def solve():
    a = 3
    b = 4
    result = a**2 + b**2
    return result
```

**æ—¥å¿—è¯æ®**:

```
Step 5 æ—¥å¿—:
```python
def solve():
    # Define the values of a and b
    a = 3
    b = 4

    # Calculate a^2 + b^2
    result = a**2 + b**2

    # Return the result
    return result
```

**åæœ**:

1. `_parse_workflow_code()` æŸ¥æ‰¾ `"class Workflow:"` å¤±è´¥
2. è¿”å›é»˜è®¤fallbackå·¥ä½œæµ
3. Fallbackä½¿ç”¨gpt-4o-miniæˆåŠŸæ±‚è§£
4. RLæ¨¡å‹è·å¾—æœ¬ä¸è¯¥æœ‰çš„å¥–åŠ±
5. å­¦ä¹ ä¿¡å·å®Œå…¨é”™è¯¯

### 3.2 æ ¹æœ¬åŸå› åˆ†æ

#### åŸå› 1: é¢„è®­ç»ƒåå·®

**Qwen2.5-7B-Instructçš„è®­ç»ƒæ•°æ®ä¸­**:

```python
# å¸¸è§æ¨¡å¼ (å æ¯”90%+):
"é—®é¢˜: è®¡ç®— 3^2 + 4^2"
"ç­”æ¡ˆ:"
def solve():
    return 3**2 + 4**2

# ç¨€æœ‰æ¨¡å¼ (å æ¯”<1%):
"é—®é¢˜: ..."
"ç”ŸæˆWorkflowç±»:"
class Workflow:
    def __init__(...): ...
    async def __call__(...): ...
```

æ¨¡å‹å­¦åˆ°çš„å¼ºå…ˆéªŒ: **é—®é¢˜ â†’ è§£é¢˜å‡½æ•°**

#### åŸå› 2: æç¤ºè¯è®¾è®¡é—®é¢˜

**å°è¯•çš„æç¤ºè¯æ¼”å˜**:

```python
# ç‰ˆæœ¬1 (å¤æ‚few-shot):
"""
# Task: Generate Python Workflow Class

Available Operators:
- Custom: ...
- AnswerGenerate: ...

## Example Workflow:
```python
class Workflow:
    def __init__(...): ...
    async def __call__(...): ...
```

Generate a Workflow class that:
1. Imports required modules
2. Initializes operators
...

```python
class Workflow:
"""

# é—®é¢˜: å¤ªå¤æ‚ï¼Œæ¨¡å‹æ··æ·†
```

```python
# ç‰ˆæœ¬2 (å½“å‰ - è¶…ç®€åŒ–):
"""
Complete the following Python Workflow class.

import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance

class Workflow:
    def __init__(self, name, llm_config, dataset):
        self.name = name
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        # Use operators to solve: {problem}
        solution = await self.custom(input=problem, ...)
        return solution['response'], cost
"""

# é—®é¢˜: æä¾›äº†å®Œæ•´æ¨¡æ¿ï¼Œä½†æ¨¡å‹ä»ç„¶åå‘ç”Ÿæˆdef solve()
```

#### åŸå› 3: æ¸©åº¦å‚æ•°

```python
# æ—§é…ç½®:
temperature: 0.7  # é«˜éšæœºæ€§ï¼Œå®¹æ˜“åç¦»æŒ‡ä»¤

# æ–°é…ç½®:
temperature: 0.1  # ä½éšæœºæ€§ï¼Œæ›´ä¸¥æ ¼éµå¾ª

# ä½†å³ä½¿0.1ï¼Œé¢„è®­ç»ƒåå·®ä»ç„¶ä¸»å¯¼
```

#### åŸå› 4: ç¼ºå°‘å¼ºçº¦æŸ

**å½“å‰ç”Ÿæˆè¿‡ç¨‹æ²¡æœ‰**:

1. **Prefixçº¦æŸ**: å¼ºåˆ¶è¾“å‡ºä»¥`class Workflow:`å¼€å¤´
2. **Stop sequences**: åœ¨ç”Ÿæˆé”™è¯¯æ—¶æå‰åœæ­¢
3. **è¯­æ³•éªŒè¯åé¦ˆ**: ç”Ÿæˆåç«‹å³æ£€æŸ¥å¹¶é‡è¯•
4. **Chatæ¨¡æ¿**: åˆ©ç”¨Qwen2.5-Instructçš„å¯¹è¯èƒ½åŠ›

### 3.3 å¥–åŠ±ä¿¡å·é”™ä½

**å½“å‰è®­ç»ƒå¾ªç¯çš„è‡´å‘½ç¼ºé™·**:

```python
# Step 1: Qwenç”Ÿæˆé”™è¯¯æ ¼å¼
generated_code = "def solve(): ..."  # âŒ é”™è¯¯

# Step 2: è§£æå¤±è´¥ï¼Œä½¿ç”¨fallback
workflow_code = get_default_workflow()  # gpt-4o-miniç‰ˆæœ¬

# Step 3: FallbackæˆåŠŸæ‰§è¡Œ
answer = "42"  # âœ… æ­£ç¡®
reward = +8.0  # é«˜å¥–åŠ±

# Step 4: RLæ›´æ–°
# Qwenè·å¾—+8.0å¥–åŠ±ï¼Œè™½ç„¶å®ƒç”Ÿæˆçš„æ˜¯é”™è¯¯æ ¼å¼ï¼
update_policy(qwen_generated_code, reward=+8.0)  # âŒâŒâŒ

# ç»“æœ: Qwenå­¦ä¼šäº†"ç”Ÿæˆdef solve()èƒ½å¾—é«˜åˆ†"
```

**æ­£ç¡®çš„å¥–åŠ±åˆ†é…åº”è¯¥æ˜¯**:

```python
if generated_code_is_valid:
    # æ‰§è¡Œç”Ÿæˆçš„ä»£ç 
    reward = compute_reward(execution_result)
else:
    # æ ¼å¼é”™è¯¯ç»™è´Ÿå¥–åŠ±
    reward = -10.0
    # ä¸è¦æ‰§è¡Œfallbackæˆ–è€…fallbackçš„ç»“æœä¸å‚ä¸æ›´æ–°
```

---

## 4. å·²å°è¯•çš„ä¼˜åŒ–æ–¹æ¡ˆ

### 4.1 æç¤ºè¯ä¼˜åŒ–å†å²

| ç‰ˆæœ¬ | ç­–ç•¥ | ç»“æœ |
|------|------|------|
| v1 | Few-shot + è¯¦ç»†è¯´æ˜ | âŒ ä»ç”Ÿæˆdef solve() |
| v2 | è¶…ç®€åŒ–æ¨¡æ¿ | âŒ ä»ç”Ÿæˆdef solve() |
| v3 | æ¸©åº¦é™ä½ 0.7â†’0.1 | â¸ï¸ æµ‹è¯•ä¸­ |

### 4.2 ä»£ç ä¿®å¤å†å²

| é—®é¢˜ | ä¿®å¤ | æ–‡ä»¶ |
|------|------|------|
| LLM Configç±»å‹é”™è¯¯ | å…ˆåŠ è½½YAMLå†ä¼ å­—å…¸ | aflow_executor.py:49-72 |
| Entry pointå‚æ•°é”™è¯¯ | Try-catché™çº§ | aflow_executor.py:122-142 |
| æ¨¡å‹ä¸‹è½½æ…¢ | ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ | training.yaml:29 |
| æ‰¹æ¬¡å¤ªå¤§è¿­ä»£æ…¢ | 8â†’4 | training.yaml:24,14 |
| ç¼ºå°‘debugè¾“å‡º | æ·»åŠ printè¯­å¥ | rl_workflow_generator.py:216-221 |

### 4.3 é…ç½®ä¼˜åŒ–

```yaml
# ä¼˜åŒ–å‰:
rollout_batch_size: 8
num_return_sequences_in_group: 8
execution_timeout: 300
temperature: 0.7

# ä¼˜åŒ–å:
rollout_batch_size: 4        # åŠ å¿«è¿­ä»£
num_return_sequences_in_group: 4
execution_timeout: 180        # æ›´å¿«å¤±è´¥æ£€æµ‹
temperature: 0.1              # ä¸¥æ ¼éµå¾ªæŒ‡ä»¤
```

---

## 5. è¯Šæ–­è®¡åˆ’

### 5.1 æ·»åŠ Debugè¾“å‡º

**å·²æ·»åŠ ** (`rl_workflow_generator.py:216-221`):

```python
def _parse_workflow_code(generated_text, problem_type):
    print(f"\n{'='*60}")
    print(f"ğŸ” DEBUG: Qwen ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬:")
    print(f"{'='*60}")
    print(generated_text[:500])
    print(f"{'='*60}\n")

    # è§£æä»£ç ...
```

**éœ€è¦é‡å¯è®­ç»ƒæ‰èƒ½çœ‹åˆ°è¾“å‡º**

### 5.2 è¯Šæ–­æ­¥éª¤

1. **é‡å¯è®­ç»ƒ** (åº”ç”¨debugä¿®æ”¹)
2. **è§‚å¯Ÿç¬¬ä¸€ä¸ªç”Ÿæˆæ ·æœ¬**
3. **åˆ†æåŸå§‹è¾“å‡º**:
   - æ˜¯å¦åŒ…å« `class Workflow:`ï¼Ÿ
   - æ˜¯å¦åŒ…å« markdownä»£ç å—ï¼Ÿ
   - æ˜¯å¦æœ‰é¢å¤–çš„è§£é‡Šæ–‡æœ¬ï¼Ÿ
4. **æ ¹æ®è§‚å¯Ÿè°ƒæ•´ç­–ç•¥**

---

## 6. å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ

### 6.1 æ–¹æ¡ˆA: å¼ºåˆ¶å‰ç¼€ç”Ÿæˆ

```python
# åœ¨generate_workflowä¸­æ·»åŠ :
from transformers import LogitsProcessor

class PrefixConstraint(LogitsProcessor):
    def __init__(self, tokenizer, prefix_text):
        self.prefix_ids = tokenizer.encode(prefix_text, add_special_tokens=False)
        self.position = 0

    def __call__(self, input_ids, scores):
        if self.position < len(self.prefix_ids):
            # å¼ºåˆ¶ä¸‹ä¸€ä¸ªtokenå¿…é¡»æ˜¯prefixçš„ä¸€éƒ¨åˆ†
            scores.fill_(-float('inf'))
            scores[:, self.prefix_ids[self.position]] = 0
            self.position += 1
        return scores

# ä½¿ç”¨:
outputs = model.generate(
    **inputs,
    logits_processor=[PrefixConstraint(tokenizer, "class Workflow:")],
    ...
)
```

**ä¼˜ç‚¹**: ä¿è¯è¾“å‡ºä»¥æ­£ç¡®æ ¼å¼å¼€å¤´
**ç¼ºç‚¹**: å¯èƒ½ç”Ÿæˆä¸å®Œæ•´çš„ä»£ç 

### 6.2 æ–¹æ¡ˆB: ä½¿ç”¨Chatæ¨¡æ¿

```python
def _build_generation_prompt(problem, problem_type):
    # Qwen2.5-Instructçš„chatæ ¼å¼
    messages = [
        {
            "role": "system",
            "content": "You are a Python code generator. Generate only valid, executable code without explanations."
        },
        {
            "role": "user",
            "content": f"""Generate a complete Workflow class to solve: {problem}

Required format:
```python
class Workflow:
    def __init__(self, name, llm_config, dataset):
        # Initialize operators
        pass

    async def __call__(self, problem: str):
        # Solve the problem
        pass
```

Generate the code now:"""
        }
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False)
    return prompt
```

**ä¼˜ç‚¹**: åˆ©ç”¨æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›
**ç¼ºç‚¹**: éœ€è¦æµ‹è¯•chatæ¨¡æ¿æ ¼å¼

### 6.3 æ–¹æ¡ˆC: ä¿®æ­£å¥–åŠ±ä¿¡å·

```python
# grpo_trainer.py:206-215
if metadata['success']:
    # åªæœ‰å½“ç”Ÿæˆçš„ä»£ç æœ‰æ•ˆæ—¶æ‰è®¡ç®—å¥–åŠ±
    if result['valid']:  # æ–°å¢æ£€æŸ¥
        reward = self.reward_computer.compute_reward(...)
    else:
        # ä»£ç æ— æ•ˆï¼Œç»™è´Ÿå¥–åŠ±
        reward = -10.0
        print(f"âš ï¸  ä»£ç æ— æ•ˆ: {result['error']}")
else:
    reward = -10.0
```

**ä¼˜ç‚¹**: ä¿®æ­£å­¦ä¹ ä¿¡å·ï¼Œé¿å…é”™è¯¯å¼ºåŒ–
**ç¼ºç‚¹**: éœ€è¦ç¡®ä¿validæ ‡å¿—å‡†ç¡®

### 6.4 æ–¹æ¡ˆD: å¤šè½®ç”Ÿæˆ + éªŒè¯

```python
def generate_workflow_with_retry(problem, problem_type, max_retries=3):
    for attempt in range(max_retries):
        result = generate_workflow(problem, problem_type)

        if result['valid']:
            return result

        # å¦‚æœå¤±è´¥ï¼Œé™ä½æ¸©åº¦é‡è¯•
        temperature = 0.1 / (attempt + 1)

    # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤
    return {
        "workflow_code": get_default_workflow(problem_type),
        "valid": False,
        "error": "Max retries exceeded"
    }
```

**ä¼˜ç‚¹**: å¢åŠ æˆåŠŸæ¦‚ç‡
**ç¼ºç‚¹**: å¢åŠ è®¡ç®—æˆæœ¬

### 6.5 æ–¹æ¡ˆE: è¯¾ç¨‹å­¦ä¹ 

```python
# é˜¶æ®µ1: åªè¦æ±‚ç”Ÿæˆç±»ç»“æ„ (ç®€å•)
prompt_phase1 = "Generate a Workflow class with __init__ and __call__ methods"

# é˜¶æ®µ2: è¦æ±‚ä½¿ç”¨ä¸€ä¸ªç®—å­ (ä¸­ç­‰)
prompt_phase2 = "Generate a Workflow class using Custom operator"

# é˜¶æ®µ3: è¦æ±‚ä¼˜åŒ–ç®—å­ç»„åˆ (å›°éš¾)
prompt_phase3 = "Generate an optimized Workflow class using 2-3 operators"
```

**ä¼˜ç‚¹**: æ¸è¿›å¼å­¦ä¹ ï¼Œé¿å…ä¸€å¼€å§‹ä»»åŠ¡å¤ªéš¾
**ç¼ºç‚¹**: éœ€è¦é‡æ–°è®¾è®¡è®­ç»ƒæµç¨‹

---

## 7. è®­ç»ƒçŠ¶æ€æ€»ç»“

### 7.1 å½“å‰è¿è¡Œå‚æ•°

```
è¿›ç¨‹PID: 2148153
å½“å‰æ­¥æ•°: Step 2/500
é¢„è®¡å®Œæˆæ—¶é—´: ~62å°æ—¶ (2.6å¤©)
æ¯æ­¥è€—æ—¶: ~7.5åˆ†é’Ÿ
GPUä½¿ç”¨: 2-3 (ç‰©ç†)

åº”ç”¨çš„ä¼˜åŒ–:
âœ… ç®€åŒ–æç¤ºè¯
âœ… æ¸©åº¦é™ä½ (0.7â†’0.1)
âœ… Debugè¾“å‡º
â¸ï¸  éœ€è¦é‡å¯æ‰èƒ½çœ‹åˆ°debugè¾“å‡º
```

### 7.2 è®­ç»ƒè¿›å±•

| Step | çŠ¶æ€ | å¹³å‡å¥–åŠ± | æœ€å¤§å¥–åŠ± | é—®é¢˜ |
|------|------|----------|----------|------|
| 1 | âœ… | -0.0000 | 8.0125 | ä½¿ç”¨æ—§æç¤ºè¯ |
| 2 | ğŸ”„ | - | - | è¿›è¡Œä¸­ |

### 7.3 å·²çŸ¥é—®é¢˜

1. âŒ **Qwenç”Ÿæˆæ ¼å¼é”™è¯¯** - æ ¸å¿ƒé—®é¢˜
2. âŒ **å¥–åŠ±ä¿¡å·é”™ä½** - fallbackæˆåŠŸâ†’Qwenè·å¥–åŠ±
3. â¸ï¸  **Debugè¾“å‡ºæœªæ¿€æ´»** - éœ€è¦é‡å¯
4. âš ï¸  **æ— æ³•éªŒè¯ä¼˜åŒ–æ•ˆæœ** - å½“å‰è¿è¡Œä½¿ç”¨æ—§ä»£ç 

---

## 8. å»ºè®®è¡ŒåŠ¨æ–¹æ¡ˆ

### 8.1 ç«‹å³è¡ŒåŠ¨ (ç´§æ€¥)

1. **åœæ­¢å½“å‰è®­ç»ƒ**
   ```bash
   kill 2148153
   ```

2. **åº”ç”¨æ–¹æ¡ˆC (ä¿®æ­£å¥–åŠ±)**
   ```python
   # grpo_trainer.py:206-215
   # æ·»åŠ  if result['valid'] æ£€æŸ¥
   ```

3. **é‡å¯è®­ç»ƒ**
   - åº”ç”¨debugè¾“å‡º
   - åº”ç”¨å¥–åŠ±ä¿®æ­£
   - è§‚å¯Ÿå‰3ä¸ªStepçš„ç”Ÿæˆè´¨é‡

### 8.2 çŸ­æœŸä¼˜åŒ– (1-2å¤©)

1. **å®æ–½æ–¹æ¡ˆB (Chatæ¨¡æ¿)**
   - ä¿®æ”¹ `_build_generation_prompt`
   - æµ‹è¯•å•ä¸ªæ ·æœ¬ç”Ÿæˆ
   - å¦‚æœæœ‰æ•ˆï¼Œåº”ç”¨åˆ°è®­ç»ƒ

2. **å®æ–½æ–¹æ¡ˆA (å‰ç¼€çº¦æŸ)**
   - ä½œä¸ºChatæ¨¡æ¿çš„è¡¥å……
   - ä¿è¯è¾“å‡ºæ ¼å¼

3. **æ”¶é›†è¯Šæ–­æ•°æ®**
   - ä¿å­˜å‰50ä¸ªStepçš„ç”Ÿæˆæ ·æœ¬
   - åˆ†æå¤±è´¥æ¨¡å¼
   - ç»Ÿè®¡æ ¼å¼æ­£ç¡®ç‡

### 8.3 ä¸­æœŸæ”¹è¿› (3-7å¤©)

1. **å¦‚æœChatæ¨¡æ¿æœ‰æ•ˆ**
   - ç»§ç»­è®­ç»ƒåˆ°Step 100
   - è¯„ä¼°LoRAæƒé‡çš„æ”¹è¿›
   - åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•

2. **å¦‚æœä»ç„¶å¤±è´¥**
   - è€ƒè™‘æ–¹æ¡ˆE (è¯¾ç¨‹å­¦ä¹ )
   - æˆ–è€…åˆ‡æ¢åˆ°æ›´å¼ºçš„åŸºåº§æ¨¡å‹ (å¦‚Qwen2.5-14B)

### 8.4 é•¿æœŸç›®æ ‡

1. **è®­ç»ƒåˆ°Step 500**
2. **åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°**
3. **ä¸å›ºå®šå·¥ä½œæµbaselineå¯¹æ¯”**
4. **åˆ†æå­¦åˆ°çš„å·¥ä½œæµæ¨¡å¼**

---

## 9. æŠ€æœ¯ç»†èŠ‚è¡¥å……

### 9.1 GRPO vs PPO

**GRPO (Group Relative Policy Optimization)**:

```python
# PPO: ä½¿ç”¨å…¨å±€baseline
baseline = mean(all_rewards)
advantages = rewards - baseline

# GRPO: ä½¿ç”¨ç»„å†…baseline
for group in groups:  # æ¯ä¸ªé—®é¢˜çš„Kä¸ªå·¥ä½œæµä¸ºä¸€ç»„
    group_baseline = mean(group_rewards)
    group_advantages = group_rewards - group_baseline
```

**ä¼˜åŠ¿**:
- å‡å°‘æ–¹å·®ï¼ˆç»„å†…æ¯”è¾ƒæ›´å…¬å¹³ï¼‰
- ä¸å—é—®é¢˜éš¾åº¦å·®å¼‚å½±å“
- æ›´ç¨³å®šçš„æ¢¯åº¦

### 9.2 LoRAç»†èŠ‚

```python
# åŸå§‹å‚æ•°: 7.6B (å†»ç»“)
base_model = Qwen2.5-7B-Instruct

# LoRAå‚æ•°: 20.2M (å¯è®­ç»ƒ)
lora_config = LoraConfig(
    r=32,                    # rank
    lora_alpha=32,           # scaling factor
    target_modules=[
        "q_proj",            # Query projection
        "k_proj",            # Key projection
        "v_proj",            # Value projection
        "o_proj"             # Output projection
    ],
    lora_dropout=0.05
)

# å‰å‘ä¼ æ’­:
output = base_model(x) + lora_alpha/r * LoRA_B @ LoRA_A @ x
#        â†‘ å†»ç»“         â†‘ å¯è®­ç»ƒ (rank=32)
```

### 9.3 æ··åˆåŸŸé‡‡æ ·

```python
# data_manager.py (æ¨æ–­)
def sample_batch(batch_size=4):
    samples = []
    for _ in range(batch_size):
        # æŒ‰æ¯”ä¾‹éšæœºé€‰æ‹©åŸŸ
        domain = np.random.choice(
            ['math', 'code', 'qa'],
            p=[0.4, 0.3, 0.3]
        )
        # ä»è¯¥åŸŸé‡‡æ ·ä¸€ä¸ªé—®é¢˜
        sample = sample_from_domain(domain)
        samples.append(sample)

    return samples
```

**å¥½å¤„**:
- é¿å…é—å¿˜ (catastrophic forgetting)
- å­¦åˆ°é€šç”¨çš„å·¥ä½œæµè®¾è®¡èƒ½åŠ›
- é€‚åº”å¤šç§é—®é¢˜ç±»å‹

---

## 10. å¤±è´¥æ¡ˆä¾‹åˆ†æ

### 10.1 Step 1 å¤±è´¥æ ·æœ¬

**é—®é¢˜**: "Solve 2x + 5 = 15"

**Qwenç”Ÿæˆ** (æ¨æ–­):
```python
def solve():
    # Step 1: Subtract 5 from both sides
    left_side = 2 * x
    right_side = 15 - 5  # = 10

    # Step 2: Divide by 2
    x = right_side / 2  # = 5

    return x
```

**è§£æç»“æœ**:
- æŸ¥æ‰¾ `"class Workflow:"` â†’ å¤±è´¥
- è¿”å› `default_workflow`
- valid = False

**æ‰§è¡Œ**:
- ä½¿ç”¨ FallbackWorkflow
- è°ƒç”¨ gpt-4o-mini
- æˆåŠŸæ±‚è§£: "x = 5"

**å¥–åŠ±**:
- correctness: 1.0 (æ­£ç¡®)
- efficiency: -0.0001 (æˆæœ¬ä½)
- simplicity: 0.9 (åªç”¨ä¸€ä¸ªç®—å­)
- **total: +8.0**

**RLæ›´æ–°**:
- Qwenè·å¾— +8.0 å¥–åŠ±
- âŒ ä½†å®ƒç”Ÿæˆçš„æ˜¯é”™è¯¯æ ¼å¼ï¼
- å¼ºåŒ–äº†é”™è¯¯è¡Œä¸º

### 10.2 ç†æƒ³æƒ…å†µ

**Qwenåº”è¯¥ç”Ÿæˆ**:
```python
import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.answer_generate = operator.AnswerGenerate(self.llm)

    async def __call__(self, problem: str):
        result = await self.answer_generate(input=problem)
        return result['answer'], self.llm.get_usage_summary()["total_cost"]
```

**è§£æç»“æœ**:
- æŸ¥æ‰¾ `"class Workflow:"` â†’ æˆåŠŸ
- è¯­æ³•æ£€æŸ¥ â†’ é€šè¿‡
- valid = True

**æ‰§è¡Œ**:
- ä½¿ç”¨ç”Ÿæˆçš„Workflow
- è°ƒç”¨ gpt-4o-mini çš„ AnswerGenerate
- æˆåŠŸæ±‚è§£

**å¥–åŠ±**:
- +8.0 (åŒæ ·çš„ç­”æ¡ˆè´¨é‡)

**RLæ›´æ–°**:
- Qwenè·å¾— +8.0 å¥–åŠ±
- âœ… æ ¼å¼æ­£ç¡®ï¼Œå¥–åŠ±æ­£ç¡®
- å¼ºåŒ–äº†æ­£ç¡®è¡Œä¸º

---

## 11. æ€§èƒ½æŒ‡æ ‡

### 11.1 è®¡ç®—æˆæœ¬

```
å•æ­¥è®­ç»ƒ:
- é—®é¢˜æ•°: 4
- æ¯ä¸ªé—®é¢˜å·¥ä½œæµæ•°: 4
- æ€»å·¥ä½œæµ: 16

æ¯ä¸ªå·¥ä½œæµ:
- ç”Ÿæˆ: ~15ç§’ (Qwen2.5-7B)
- æ‰§è¡Œ: ~30ç§’ (gpt-4o-mini APIè°ƒç”¨)
- å¥–åŠ±è®¡ç®—: <1ç§’
- ç­–ç•¥æ›´æ–°: ~20ç§’

æ€»è®¡: ~7.5åˆ†é’Ÿ/step

å®Œæ•´è®­ç»ƒ:
- 500 steps Ã— 7.5 min = 3750 min â‰ˆ 62.5 hours â‰ˆ 2.6 days
```

### 11.2 GPUå†…å­˜

```
Qwen2.5-7B (bfloat16):
- æ¨¡å‹å‚æ•°: 7.6B Ã— 2 bytes = 15.2 GB
- æ¿€æ´»å€¼: ~5 GB (batch_size=1, seq_len=4096)
- LoRAå‚æ•°: 20M Ã— 4 bytes = 80 MB
- æ¢¯åº¦: 80 MB

æ€»è®¡: ~21 GB / GPU
ä½¿ç”¨: 2 Ã— RTX 3090 (24GB each)
```

### 11.3 APIæˆæœ¬

```
gpt-4o-miniå®šä»·:
- Input: $0.150 / 1M tokens
- Output: $0.600 / 1M tokens

å•ä¸ªæ‰§è¡Œ:
- Input: ~50 tokens
- Output: ~200 tokens
- Cost: ~$0.0001

å®Œæ•´è®­ç»ƒ:
- 500 steps Ã— 16 workflows = 8000 executions
- Total cost: ~$0.8
```

---

## 12. æ€»ç»“ä¸å±•æœ›

### 12.1 æ ¸å¿ƒæŒ‘æˆ˜

1. **æ¨¡å‹èƒ½åŠ›è¾¹ç•Œ**: Qwen2.5-7Bå¯èƒ½ç¼ºä¹ç”Ÿæˆå¤æ‚ç±»ç»“æ„çš„èƒ½åŠ›
2. **é¢„è®­ç»ƒåå·®**: å¼ºçƒˆå€¾å‘äºç”Ÿæˆè§£é¢˜å‡½æ•°
3. **æç¤ºè¯å·¥ç¨‹**: éœ€è¦æ‰¾åˆ°èƒ½æ¿€å‘æ­£ç¡®è¡Œä¸ºçš„æç¤ºæ ¼å¼
4. **å¥–åŠ±å¯¹é½**: å¿…é¡»ç¡®ä¿å¥–åŠ±ä¸å®é™…ç”Ÿæˆè´¨é‡ä¸€è‡´

### 12.2 æˆåŠŸæ¡ä»¶

è¦ä½¿è®­ç»ƒæˆåŠŸï¼Œéœ€è¦æ»¡è¶³:

1. âœ… **Qwenç”Ÿæˆæ ¼å¼æ­£ç¡®ç‡ > 80%**
2. âœ… **å¥–åŠ±ä¿¡å·å‡†ç¡®** (æ ¼å¼é”™è¯¯â†’è´Ÿå¥–åŠ±)
3. âœ… **ç”Ÿæˆçš„å·¥ä½œæµèƒ½æ‰§è¡Œ**
4. âœ… **æ‰§è¡Œç»“æœä¼˜äºbaseline**

### 12.3 Plan B

å¦‚æœQwen2.5-7BæŒç»­å¤±è´¥:

1. **å‡çº§æ¨¡å‹**: Qwen2.5-14B æˆ– Qwen2.5-32B
2. **ç®€åŒ–ä»»åŠ¡**: åªä¼˜åŒ–ç®—å­é€‰æ‹©ï¼Œä¸ç”Ÿæˆå®Œæ•´ç±»
3. **æ··åˆæ–¹æ³•**: æ¨¡æ¿ + RLå¡«ç©º
4. **ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒ**: å…ˆåœ¨åˆæˆæ•°æ®ä¸ŠSFT

---

## é™„å½•A: å…³é”®ä»£ç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶ | è¡Œæ•° |
|------|------|------|
| GRPOä¸»å¾ªç¯ | grpo_trainer.py | 393-423 |
| å•æ­¥è®­ç»ƒ | grpo_trainer.py | 145-259 |
| Workflowç”Ÿæˆ | rl_workflow_generator.py | 177-247 |
| æç¤ºè¯æ„å»º | rl_workflow_generator.py | 113-139 |
| ä»£ç è§£æ | rl_workflow_generator.py | 213-253 |
| å·¥ä½œæµæ‰§è¡Œ | aflow_executor.py | 74-196 |
| Fallback | aflow_executor.py | 251-282 |
| ç­–ç•¥æ›´æ–° | grpo_trainer.py | 287-368 |
| å¥–åŠ±è®¡ç®— | reward_computer.py | (æœªè¯»å–ï¼Œæ¨æ–­æ¥å£) |

## é™„å½•B: é…ç½®æ–‡ä»¶

**training.yaml**: `/home/yijia/.claude/11/integrated_aflow_roll/config/training.yaml`
**aflow_llm.yaml**: `/home/yijia/.claude/11/integrated_aflow_roll/config/aflow_llm.yaml`

## é™„å½•C: æ—¥å¿—åˆ†æå‘½ä»¤

```bash
# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
tail -f logs/training_output.log

# æŸ¥æ‰¾ç”Ÿæˆçš„ä»£ç 
grep -A 20 "def solve" logs/training_output.log

# æŸ¥æ‰¾å¥–åŠ±
grep "avg_reward" logs/training_output.log

# æŸ¥æ‰¾é”™è¯¯
grep -E "é”™è¯¯|Error|Exception" logs/training_output.log

# æŸ¥çœ‹GPUä½¿ç”¨
nvidia-smi
```

---

**æŠ¥å‘Šç»“æŸ**
**å»ºè®®ä¼˜å…ˆçº§**: åœæ­¢è®­ç»ƒ â†’ ä¿®æ­£å¥–åŠ±ä¿¡å· â†’ æµ‹è¯•Chatæ¨¡æ¿ â†’ é‡å¯è®­ç»ƒ
