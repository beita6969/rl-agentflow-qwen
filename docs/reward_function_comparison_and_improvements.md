# å¥–åŠ±å‡½æ•°å¯¹æ¯”åˆ†æä¸æ”¹è¿›æ–¹æ¡ˆ

## ğŸ“Š å½“å‰è®­ç»ƒçŠ¶æ€ (2025-11-16)

**è®­ç»ƒè¿›åº¦:** Step 3/500 (0.6%)

**å‡†ç¡®ç‡ç»Ÿè®¡:**
- **Step 1:** 3/16 = 18.8%
- **Step 2:** 6/16 = 37.5% âœ… **æå‡19%**
- **Step 3:** æ•°æ®æ”¶é›†ä¸­...

**è´¨é‡æŒ‡æ ‡:**
- æ ¼å¼æ­£ç¡®ç‡: 97.1% (34/35)
- Fallbackä½¿ç”¨ç‡: 0.0%
- eval()é”™è¯¯ç‡: 8.6%
- å¹³å‡APIæˆæœ¬: $0.000091/è°ƒç”¨

**ç»“è®º:** è®­ç»ƒåˆšå¼€å§‹,æ¨¡å‹æ­£åœ¨å¿«é€Ÿå­¦ä¹ ä¸­,å‡†ç¡®ç‡ä»18.8%æå‡åˆ°37.5%æ˜¾ç¤ºäº†è‰¯å¥½çš„å­¦ä¹ è¶‹åŠ¿ã€‚

---

## ğŸ” å¥–åŠ±å‡½æ•°å¯¹æ¯”åˆ†æ

### 1. å½“å‰ç³»ç»Ÿ (integrated_aflow_roll)

**æ–‡ä»¶:** `src/reward_computer.py` (185è¡Œ)

**å¥–åŠ±å…¬å¼:**
```python
total_reward = 0.7 Ã— correctness + 0.2 Ã— efficiency + 0.1 Ã— simplicity
```

**ç»´åº¦åˆ†æ:**

| ç»´åº¦ | æƒé‡ | èŒƒå›´ | è®¡ç®—æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|---------|------|------|
| **Correctness** | 70% | [-10, 10] | æ•°å­¦:æ•°å­—åŒ¹é…<br>ä»£ç :å­—ç¬¦ä¸²åŒ…å«<br>QA:Tokené‡å  | ç®€å•å¿«é€Ÿ | ç²—ç²’åº¦è¯„ä¼° |
| **Efficiency** | 20% | [-8, 10] | åŸºäºAPIæˆæœ¬åˆ†çº§ | é¼“åŠ±ä½æˆæœ¬ | æœªè€ƒè™‘æ‰§è¡Œæ—¶é—´ |
| **Simplicity** | 10% | [-5, 10] | æ‰§è¡Œæ—¶é—´+ç®—å­æ•° | é¼“åŠ±ç®€æ´ | æƒé‡å¯èƒ½è¿‡ä½ |

**ç‰¹ç‚¹:**
- âœ… ç®€å•æ˜ç¡®,æ˜“äºç†è§£
- âœ… å¿«é€Ÿè®¡ç®—,æ— éœ€é¢å¤–APIè°ƒç”¨
- âœ… å¤šç»´åº¦è¦†ç›–(ç»“æœ+æ•ˆç‡+è´¨é‡)
- âŒ æ­£ç¡®æ€§è¯„ä¼°ç²—ç³™(å­—ç¬¦ä¸²åŒ¹é…)
- âŒ æ— è¿‡ç¨‹å¥–åŠ±(process reward)
- âŒ æƒé‡ç¡¬ç¼–ç ,æ— æ³•è‡ªé€‚åº”

---

### 2. ROLLæ¡†æ¶

**æ–‡ä»¶:** `ROLL/roll/pipeline/rlvr/rewards/` (8ä¸ªWorker,4408è¡Œ)

**æ¶æ„ç‰¹ç‚¹:**
```
å¤šåŸŸè·¯ç”±ç³»ç»Ÿ
â”œâ”€â”€ MathRuleRewardWorker (272è¡Œ) - æ•°å­¦é¢˜ä¸“ç”¨
â”œâ”€â”€ CodeSandboxRewardWorker (865è¡Œ) - ä»£ç æ‰§è¡Œæ²™ç®±
â”œâ”€â”€ LLMJudgeRewardWorker (249è¡Œ) - LLMæ™ºèƒ½è¯„åˆ¤
â”œâ”€â”€ IFEvalRuleRewardWorker (700è¡Œ) - æŒ‡ä»¤éµå¾ª(27ä¸ªçº¦æŸå‡½æ•°)
â”œâ”€â”€ CrossThinkQARewardWorker (266è¡Œ) - æ¨ç†QA
â”œâ”€â”€ GeneralValRewardWorker (189è¡Œ) - é€šç”¨éªŒè¯
â”œâ”€â”€ MultipleChoiceRewardWorker (164è¡Œ) - é€‰æ‹©é¢˜
â””â”€â”€ DetectionRewardWorker (704è¡Œ) - ç›®æ ‡æ£€æµ‹
```

**MathRuleRewardWorkerè¯¦ç»†åˆ†æ:**

| ç»´åº¦ | æƒé‡ | è®¡ç®—æ–¹æ³• | ç¤ºä¾‹ |
|------|------|---------|------|
| **verify_answer** | ä¸»è¦ | math_verifyåº“éªŒè¯(LaTeX+è¡¨è¾¾å¼) | 0/1äºŒå€¼ |
| **repetition_penalty** | è¾…åŠ© | N-gramé‡å¤åº¦(ngram=3) | -0.1 max |
| **format_reward** | è¾…åŠ© | æ­£åˆ™åŒ¹é…`<think>...</think><answer>...</answer>` | 0/-1 |
| **long_block_penalty** | è¾…åŠ© | å•è¯æœ€å¤§é•¿åº¦>100 | -1 |
| **response_length** | å½’ä¸€åŒ– | len(response)/20000 | [0,1] |

**å…¬å¼:**
```python
response_level_reward = verify_answer + repetition_penalty + format_reward
token_level_reward = å…¨0 (ç¨€ç–å¥–åŠ±è®¾è®¡)
```

**CodeSandboxRewardWorkerè¯¦ç»†åˆ†æ:**

**æ ¸å¿ƒèƒ½åŠ›:** çœŸå®ä»£ç æ‰§è¡Œæµ‹è¯•

| æµ‹è¯•ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|---------|------|------|
| **Input/Output** | stdinâ†’stdoutå¯¹æ¯” | `"input": "5\n", "expected": "25\n"` |
| **Assertæµ‹è¯•** | Python assertè¯­å¥ | `assert solution(5) == 25` |
| **Pytesté›†æˆ** | å®Œæ•´pytestæ¡†æ¶ | æ”¯æŒfixtureså’Œå‚æ•°åŒ– |
| **Checkå‡½æ•°** | è‡ªå®šä¹‰æ£€æŸ¥å‡½æ•° | `def check(candidate): ...` |

**å¥–åŠ±è®¡ç®—:**
```python
reward = pass_test_ratio (é€šè¿‡æµ‹è¯•æ•°/æ€»æµ‹è¯•æ•°)
+ format_validation (0/1)
+ think_tag_check (0/1)
- error_penalty (SyntaxError=-1, LogicError=-2)
```

**é”™è¯¯åˆ†ç±»ç³»ç»Ÿ:**
- âœ… `SyntaxError`: ç¼–è¯‘é”™è¯¯ â†’ -1æƒ©ç½š
- âœ… `LogicError`: é€»è¾‘é”™è¯¯ â†’ -2æƒ©ç½š
- âœ… `ReturnCode`: è¿è¡Œæ—¶é”™è¯¯ â†’ è®°å½•code

**ROLLçš„ä¼˜åŠ¿:**
- âœ… **åŸŸç‰¹å¼‚æ€§è®¾è®¡:** ä¸ºæ¯ç§ä»»åŠ¡ç±»å‹å®šåˆ¶ä¸“ç”¨Worker
- âœ… **çœŸå®æ‰§è¡ŒéªŒè¯:** ä»£ç æ²™ç®±å®é™…è¿è¡Œæµ‹è¯•ç”¨ä¾‹
- âœ… **ç»†ç²’åº¦è¯„ä¼°:** 27ä¸ªæŒ‡ä»¤çº¦æŸå‡½æ•°
- âœ… **é…ç½®é©±åŠ¨:** é€šè¿‡tagè‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”Worker
- âœ… **å¯¹æŠ—é²æ£’æ€§:** æ£€æµ‹BPEæ”»å‡»(é•¿æ–‡æœ¬å—)
- âœ… **å¤šç»´åº¦ç»„åˆ:** æ­£ç¡®æ€§+æ ¼å¼+é‡å¤+é•¿åº¦

**ROLLçš„å±€é™:**
- âŒ **å¤æ‚åº¦é«˜:** 4408è¡Œä»£ç ,ç»´æŠ¤æˆæœ¬å¤§
- âŒ **ç¡¬ç¼–ç æƒé‡:** å„ç»´åº¦æƒé‡ä¸å¯å­¦ä¹ 
- âŒ **æ— è¿‡ç¨‹å¥–åŠ±:** token_levelå…¨ä¸º0
- âŒ **è®¡ç®—å¼€é”€:** ä»£ç æ²™ç®±æ‰§è¡Œè€—æ—¶

---

### 3. AgentFlowæ¡†æ¶

**æ–‡ä»¶:** `AgentFlow/agentflow/reward.py` (67è¡Œ)

**è®¾è®¡å“²å­¦:** è£…é¥°å™¨æ¨¡å¼ + LLMè¯„åˆ¤

**æ ¸å¿ƒä»£ç :**
```python
@reward
def compute_reward(response: str, correct_answer: str) -> float:
    """
    è£…é¥°å™¨è‡ªåŠ¨å¤„ç†:
    1. å¼‚æ­¥/åŒæ­¥å‡½æ•°å…¼å®¹
    2. è¿”å›å€¼ç±»å‹éªŒè¯(float/int/None)
    3. AgentOpsè¿½è¸ªé›†æˆ
    4. RewardSpanDataå°è£…
    """
    return 1.0 if response == correct_answer else 0.0
```

**LLMè¯„åˆ¤ç³»ç»Ÿ (calculate_score_unified.py):**

```python
class ResultScorer:
    def __init__(self):
        # ä½¿ç”¨GPT-4oä½œä¸ºè¯„åˆ¤æ¨¡å‹
        self.llm_engine = ChatOpenAI(
            model_string="gpt-4o",
            is_multimodal=False,
            enable_cache=True  # ç¼“å­˜ä¼˜åŒ–
        )

    def answer_verification(self, question, response, correct_answer):
        """
        è¯„åˆ¤æµç¨‹:
        1. æå–<answer>æ ‡ç­¾å†…å®¹
        2. GPT-4oåˆ¤æ–­æ˜¯å¦æ­£ç¡®
        3. è¿”å›åˆ†æ+å¸ƒå°”åˆ¤æ–­
        """
        prompt = f"""
        Question: {question}
        Response: {response}
        Correct Answer: {correct_answer}

        Is the response correct? (Yes/No)
        Provide analysis.
        """

        llm_result = self.llm_engine.call(prompt)
        return parse_yes_no(llm_result)
```

**å¹¶è¡Œè¯„åˆ†ç³»ç»Ÿ:**
```python
def score_results(self, results, max_workers=10):
    """
    ç‰¹ç‚¹:
    - ThreadPoolExecutorå¹¶è¡Œå¤„ç†
    - æœ€å¤š10ä¸ªworkeråŒæ—¶è¯„åˆ†
    - è¿›åº¦æ¡å®æ—¶æ˜¾ç¤º
    - æ”¯æŒç¼“å­˜é¿å…é‡å¤è°ƒç”¨
    """
    with ThreadPoolExecutor(max_workers) as executor:
        futures = [
            executor.submit(self.answer_verification, r)
            for r in results
        ]

        for future in tqdm(as_completed(futures)):
            result = future.result()
            # ç»Ÿè®¡æ­£ç¡®/é”™è¯¯
```

**AgentFlowçš„ä¼˜åŠ¿:**
- âœ… **æç®€è®¾è®¡:** æ ¸å¿ƒåªæœ‰67è¡Œä»£ç 
- âœ… **æ™ºèƒ½è¯„åˆ¤:** GPT-4oç†è§£å¤æ‚æ¨ç†
- âœ… **é«˜æ‰©å±•æ€§:** è£…é¥°å™¨æ¨¡å¼æ˜“äºæ·»åŠ æ–°å¥–åŠ±
- âœ… **æ·±åº¦é›†æˆ:** ä¸AgentOpsæ— ç¼è¿½è¸ª
- âœ… **ç¼“å­˜ä¼˜åŒ–:** é¿å…é‡å¤LLMè°ƒç”¨
- âœ… **å¹¶è¡Œå¤„ç†:** 10 workersåŒæ—¶è¯„åˆ†

**AgentFlowçš„å±€é™:**
- âŒ **APIä¾èµ–:** éœ€è¦è°ƒç”¨GPT-4o,æˆæœ¬é«˜
- âŒ **ç¦»çº¿è¯„ä¼°:** ä¸é€‚åˆåœ¨çº¿RLè®­ç»ƒ
- âŒ **å•ä¸€ç»´åº¦:** ä¸»è¦åªæœ‰æ­£ç¡®æ€§è¯„åˆ¤
- âŒ **å»¶è¿Ÿé—®é¢˜:** LLMè°ƒç”¨è€—æ—¶(2-5ç§’/æ ·æœ¬)

---

## ğŸ“ˆ ä¸‰è€…å¯¹æ¯”æ€»ç»“è¡¨

| ç‰¹æ€§ | å½“å‰ç³»ç»Ÿ | ROLL | AgentFlow |
|------|---------|------|-----------|
| **ä»£ç è§„æ¨¡** | 185è¡Œ | 4408è¡Œ | 67è¡Œ |
| **å¥–åŠ±ç»´åº¦** | 3ä¸ª(å›ºå®š) | 5-8ä¸ª(å¯é…ç½®) | 1ä¸ª(å¯æ‰©å±•) |
| **è¯„ä¼°æ–¹æ³•** | è§„åˆ™åŒ¹é… | è§„åˆ™+æ²™ç®±æ‰§è¡Œ | LLMè¯„åˆ¤ |
| **è®¡ç®—é€Ÿåº¦** | å¿«(msçº§) | ä¸­(ç§’çº§) | æ…¢(2-5ç§’) |
| **å‡†ç¡®æ€§** | ä¸­ç­‰ | é«˜ | æœ€é«˜ |
| **é€‚ç”¨åœºæ™¯** | åœ¨çº¿RLè®­ç»ƒ | å¤§è§„æ¨¡å¤šåŸŸRL | ç¦»çº¿åˆ†æ |
| **APIæˆæœ¬** | ä½ | ä½ | é«˜(GPT-4o) |
| **å¯æ‰©å±•æ€§** | ä¸­ | ä½(éœ€åŠ Worker) | é«˜(è£…é¥°å™¨) |
| **è¿‡ç¨‹å¥–åŠ±** | æ—  | æ— (token_levelå…¨0) | å¯é€‰ |
| **æƒé‡å­¦ä¹ ** | æ—  | æ—  | æ—  |
| **å¤šåŸŸæ”¯æŒ** | æ‰‹åŠ¨if | è‡ªåŠ¨è·¯ç”± | æ‰‹åŠ¨æ³¨å†Œ |

---

## ğŸš€ æ”¹è¿›å»ºè®®æ–¹æ¡ˆ

### æ–¹æ¡ˆA: æ¸è¿›å¼æ”¹è¿›(æ¨èä¼˜å…ˆå®æ–½)

**ç›®æ ‡:** åœ¨å½“å‰ç³»ç»ŸåŸºç¡€ä¸Š,å¸æ”¶ROLLå’ŒAgentFlowçš„ä¼˜ç‚¹

#### A1. æ”¹è¿›æ­£ç¡®æ€§è¯„ä¼°(å€Ÿé‰´ROLL)

**å½“å‰é—®é¢˜:** æ•°å­¦é¢˜åªæå–æœ€åä¸€ä¸ªæ•°å­—,ä»£ç é¢˜åªåšå­—ç¬¦ä¸²åŒ…å«

**æ”¹è¿›æ–¹æ¡ˆ:**
```python
class ImprovedCorrectnessEvaluator:
    """æ”¹è¿›çš„æ­£ç¡®æ€§è¯„ä¼°å™¨"""

    def __init__(self):
        # æ·»åŠ math_verifyåº“
        from sympy import sympify, simplify
        self.math_verifier = self._math_verify

    def _math_verify(self, pred_str: str, gt_str: str) -> float:
        """
        ROLLé£æ ¼çš„æ•°å­¦éªŒè¯
        æ”¯æŒ:
        1. æ•°å­—æå–(ä¿ç•™å½“å‰æ–¹æ³•)
        2. LaTeXè¡¨è¾¾å¼è§£æ
        3. ç¬¦å·è¡¨è¾¾å¼éªŒè¯
        """
        try:
            # æ–¹æ³•1: æ•°å­—æå–(å¿«é€Ÿ)
            pred_nums = self._extract_numbers(pred_str)
            gt_nums = self._extract_numbers(gt_str)

            if pred_nums and gt_nums:
                if abs(pred_nums[-1] - gt_nums[-1]) < 1e-4:
                    return 10.0

            # æ–¹æ³•2: LaTeXè§£æ(ç²¾ç¡®)
            pred_expr = self._parse_latex(pred_str)
            gt_expr = self._parse_latex(gt_str)

            if pred_expr and gt_expr:
                if simplify(pred_expr - gt_expr) == 0:
                    return 10.0

            # æ–¹æ³•3: å­—ç¬¦ä¸²ç›¸ä¼¼åº¦(å…œåº•)
            if self._string_similarity(pred_str, gt_str) > 0.9:
                return 8.0

            return -5.0

        except Exception as e:
            # é™çº§åˆ°åŸå§‹æ–¹æ³•
            return self._original_math_correctness(pred_str, gt_str)

    def _parse_latex(self, text: str):
        """æå–å¹¶è§£æLaTeXè¡¨è¾¾å¼"""
        # æå– \boxed{...} æˆ– $...$
        import re

        boxed = re.search(r'\\boxed\{([^}]+)\}', text)
        if boxed:
            return sympify(boxed.group(1))

        dollar = re.search(r'\$([^$]+)\$', text)
        if dollar:
            return sympify(dollar.group(1))

        return None
```

**é¢„æœŸæ•ˆæœ:**
- âœ… æ”¯æŒæ›´å¤šæ•°å­¦è¡¨è¾¾å¼æ ¼å¼
- âœ… æå‡æ•°å­¦é¢˜è¯„ä¼°å‡†ç¡®æ€§
- âœ… ä¿æŒå¿«é€Ÿè®¡ç®—é€Ÿåº¦
- **å®æ–½æˆæœ¬:** ä½(1-2å°æ—¶)

---

#### A2. æ·»åŠ æ ¼å¼å¥–åŠ±ç»´åº¦(å€Ÿé‰´ROLL)

**å½“å‰é—®é¢˜:** æ— æ ¼å¼æ£€æŸ¥,æ¨¡å‹å¯èƒ½ç”Ÿæˆæ··ä¹±è¾“å‡º

**æ”¹è¿›æ–¹æ¡ˆ:**
```python
def _compute_format_reward(self, response: str, problem_type: str) -> float:
    """
    æ£€æŸ¥å“åº”æ ¼å¼è§„èŒƒæ€§

    è¿”å›:
        +2.0: å®Œç¾æ ¼å¼
        +0.0: åŸºæœ¬æ ¼å¼
        -2.0: æ ¼å¼æ··ä¹±
    """

    if problem_type == "math":
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒè¿‡ç¨‹+ç­”æ¡ˆ
        has_think = bool(re.search(r'<think>.*?</think>', response, re.DOTALL))
        has_answer = bool(re.search(r'<answer>.*?</answer>', response, re.DOTALL))

        if has_think and has_answer:
            return 2.0
        elif has_answer:
            return 0.0
        else:
            return -2.0

    elif problem_type == "code":
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç å—
        has_code_block = bool(re.search(r'```python.*?```', response, re.DOTALL))

        if has_code_block:
            return 2.0
        else:
            return -2.0

    elif problem_type == "qa":
        # æ£€æŸ¥ç­”æ¡ˆé•¿åº¦åˆç†æ€§
        if 10 < len(response) < 500:
            return 2.0
        else:
            return 0.0

    return 0.0
```

**é›†æˆåˆ°æ€»å¥–åŠ±:**
```python
# æ›´æ–°reward_computer.pyçš„compute_rewardæ–¹æ³•
total_reward = (
    0.65 * correctness_reward +      # é™ä½5%ç»™æ ¼å¼
    0.20 * efficiency_reward +
    0.10 * simplicity_reward +
    0.05 * format_reward             # æ–°å¢5%
)
```

**é¢„æœŸæ•ˆæœ:**
- âœ… é¼“åŠ±æ¨¡å‹éµå¾ªæ ¼å¼è§„èŒƒ
- âœ… æå‡è¾“å‡ºå¯è¯»æ€§
- âœ… å‡å°‘è§£æé”™è¯¯
- **å®æ–½æˆæœ¬:** ä½(1å°æ—¶)

---

#### A3. æ·»åŠ é‡å¤æƒ©ç½š(å€Ÿé‰´ROLL)

**å½“å‰é—®é¢˜:** æ¨¡å‹å¯èƒ½ç”Ÿæˆå¤§é‡é‡å¤å†…å®¹è·å–é•¿åº¦å¥–åŠ±

**æ”¹è¿›æ–¹æ¡ˆ:**
```python
def _compute_repetition_penalty(self, response: str, ngram_size: int = 3) -> float:
    """
    è®¡ç®—N-gramé‡å¤åº¦æƒ©ç½š

    Args:
        response: å“åº”æ–‡æœ¬
        ngram_size: N-gramå¤§å°(é»˜è®¤3)

    Returns:
        æƒ©ç½šå€¼: [-2.0, 0.0]
    """
    words = response.split()

    if len(words) < ngram_size:
        return 0.0

    # ç”Ÿæˆæ‰€æœ‰N-grams
    ngrams = []
    for i in range(len(words) - ngram_size + 1):
        ngram = tuple(words[i:i+ngram_size])
        ngrams.append(ngram)

    if not ngrams:
        return 0.0

    # è®¡ç®—å”¯ä¸€N-gramsæ¯”ä¾‹
    unique_ratio = len(set(ngrams)) / len(ngrams)

    # è½¬æ¢ä¸ºæƒ©ç½š
    if unique_ratio > 0.9:
        return 0.0          # å‡ ä¹æ— é‡å¤
    elif unique_ratio > 0.7:
        return -0.5         # è½»å¾®é‡å¤
    elif unique_ratio > 0.5:
        return -1.0         # ä¸­åº¦é‡å¤
    else:
        return -2.0         # ä¸¥é‡é‡å¤
```

**é¢„æœŸæ•ˆæœ:**
- âœ… é˜²æ­¢æ¨¡å‹ç”Ÿæˆé‡å¤å†…å®¹
- âœ… é¼“åŠ±å¤šæ ·åŒ–è¡¨è¾¾
- âœ… é¿å…reward hacking
- **å®æ–½æˆæœ¬:** ä½(1å°æ—¶)

---

#### A4. å¯é€‰LLMè¯„åˆ¤(å€Ÿé‰´AgentFlow)

**åº”ç”¨åœºæ™¯:** å¯¹å‡†ç¡®ç‡è¦æ±‚æé«˜çš„åœºæ™¯,æˆ–ç¦»çº¿éªŒè¯

**æ”¹è¿›æ–¹æ¡ˆ:**
```python
class OptionalLLMJudge:
    """å¯é€‰çš„LLMè¯„åˆ¤å™¨"""

    def __init__(self, enable: bool = False, model: str = "gpt-4o-mini"):
        self.enable = enable
        self.model = model

        if enable:
            from openai import OpenAI
            self.client = OpenAI()

    def judge(self, question: str, response: str, ground_truth: str) -> float:
        """
        ä½¿ç”¨LLMè¯„åˆ¤ç­”æ¡ˆè´¨é‡

        Returns:
            [0, 10] çš„è¯„åˆ†
        """
        if not self.enable:
            return None

        prompt = f"""
You are an expert evaluator. Rate the response quality on scale 0-10.

Question: {question}

Response: {response}

Ground Truth: {ground_truth}

Evaluate based on:
1. Correctness (most important)
2. Reasoning quality
3. Clarity

Format: Score: X.X
"""

        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )

            result = completion.choices[0].message.content

            # æå–åˆ†æ•°
            import re
            match = re.search(r'Score:\s*([0-9.]+)', result)
            if match:
                return float(match.group(1))

        except Exception as e:
            print(f"LLM judge error: {e}")

        return None

# åœ¨reward_computer.pyä¸­é›†æˆ
class RewardComputer:
    def __init__(self, reward_weights, use_llm_judge: bool = False):
        self.llm_judge = OptionalLLMJudge(enable=use_llm_judge)

    def compute_reward(self, problem, prediction, ground_truth, problem_type, metadata):
        # åŸæœ‰å¥–åŠ±è®¡ç®—
        rule_based_reward = ...

        # å¯é€‰LLMè¯„åˆ¤
        if self.llm_judge.enable:
            llm_score = self.llm_judge.judge(problem, prediction, ground_truth)

            if llm_score is not None:
                # æ··åˆï¼š70% è§„åˆ™ + 30% LLM
                final_reward = 0.7 * rule_based_reward + 0.3 * llm_score
                return final_reward

        return rule_based_reward
```

**ä½¿ç”¨å»ºè®®:**
- è®­ç»ƒæ—¶: `use_llm_judge=False` (å¿«é€Ÿ)
- éªŒè¯æ—¶: `use_llm_judge=True` (å‡†ç¡®)

**é¢„æœŸæ•ˆæœ:**
- âœ… ç¦»çº¿è¯„ä¼°æ›´å‡†ç¡®
- âœ… å¯éªŒè¯è§„åˆ™å¥–åŠ±çš„è´¨é‡
- âš ï¸  è®­ç»ƒæ—¶æˆæœ¬é«˜,ä¸æ¨è
- **å®æ–½æˆæœ¬:** ä¸­(2-3å°æ—¶)

---

### æ–¹æ¡ˆB: ä»£ç æ‰§è¡ŒéªŒè¯(å€Ÿé‰´ROLL,é«˜çº§)

**ç›®æ ‡:** ä¸ºä»£ç é¢˜æ·»åŠ çœŸå®æ‰§è¡Œæµ‹è¯•

**å½“å‰é—®é¢˜:** ä»£ç é¢˜åªåšå­—ç¬¦ä¸²åŒ…å«,æ— æ³•éªŒè¯ä»£ç æ­£ç¡®æ€§

**æ”¹è¿›æ–¹æ¡ˆ:**
```python
class CodeExecutionValidator:
    """ä»£ç æ‰§è¡ŒéªŒè¯å™¨"""

    def __init__(self, timeout: int = 5):
        self.timeout = timeout

    def validate_code(self, workflow_output: str, test_cases: List[Dict]) -> float:
        """
        æ‰§è¡Œä»£ç å¹¶æµ‹è¯•

        Args:
            workflow_output: å·¥ä½œæµè¾“å‡º(åŒ…å«ä»£ç )
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
                [
                    {"input": "2 3", "expected_output": "5"},
                    {"assert": "assert solution(2, 3) == 5"}
                ]

        Returns:
            é€šè¿‡ç‡ [0, 1]
        """
        # æå–ä»£ç 
        code = self._extract_code(workflow_output)

        if not code:
            return 0.0

        # è¿è¡Œæµ‹è¯•
        passed = 0
        total = len(test_cases)

        for test in test_cases:
            try:
                if 'input' in test and 'expected_output' in test:
                    # Input/Outputæµ‹è¯•
                    result = self._run_with_io(
                        code,
                        test['input'],
                        timeout=self.timeout
                    )

                    if result.strip() == test['expected_output'].strip():
                        passed += 1

                elif 'assert' in test:
                    # Assertæµ‹è¯•
                    success = self._run_assert(
                        code,
                        test['assert'],
                        timeout=self.timeout
                    )

                    if success:
                        passed += 1

            except Exception as e:
                # æµ‹è¯•å¤±è´¥
                continue

        return passed / total if total > 0 else 0.0

    def _extract_code(self, text: str) -> str:
        """æå–ä»£ç å—"""
        import re

        # æå–```python ... ```
        match = re.search(r'```python\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            return match.group(1)

        # æå–```... ```
        match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            return match.group(1)

        return ""

    def _run_with_io(self, code: str, input_str: str, timeout: int) -> str:
        """æ‰§è¡Œä»£ç å¹¶æ•è·è¾“å‡º"""
        import subprocess
        import tempfile

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            temp_path = f.name

        try:
            # æ‰§è¡Œ
            result = subprocess.run(
                ['python3', temp_path],
                input=input_str,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            return result.stdout

        except subprocess.TimeoutExpired:
            return ""

        finally:
            import os
            os.unlink(temp_path)

    def _run_assert(self, code: str, assert_stmt: str, timeout: int) -> bool:
        """æ‰§è¡Œæ–­è¨€æµ‹è¯•"""
        import subprocess
        import tempfile

        # ç»„åˆä»£ç å’Œæ–­è¨€
        full_code = code + "\n\n" + assert_stmt

        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(full_code)
            temp_path = f.name

        try:
            result = subprocess.run(
                ['python3', temp_path],
                capture_output=True,
                timeout=timeout
            )

            # è¿”å›ç 0è¡¨ç¤ºæ–­è¨€é€šè¿‡
            return result.returncode == 0

        except:
            return False

        finally:
            import os
            os.unlink(temp_path)

# é›†æˆåˆ°RewardComputer
class RewardComputer:
    def __init__(self, reward_weights, enable_code_execution: bool = False):
        self.code_validator = CodeExecutionValidator() if enable_code_execution else None

    def _compute_code_correctness(self, prediction: str, ground_truth: str, test_cases: List[Dict] = None) -> float:
        """æ”¹è¿›çš„ä»£ç æ­£ç¡®æ€§è¯„ä¼°"""

        # æ–¹æ³•1: ä»£ç æ‰§è¡Œæµ‹è¯•(å¦‚æœæœ‰æµ‹è¯•ç”¨ä¾‹)
        if self.code_validator and test_cases:
            pass_rate = self.code_validator.validate_code(prediction, test_cases)

            if pass_rate == 1.0:
                return 10.0       # æ‰€æœ‰æµ‹è¯•é€šè¿‡
            elif pass_rate >= 0.8:
                return 7.0        # å¤§éƒ¨åˆ†é€šè¿‡
            elif pass_rate >= 0.5:
                return 4.0        # ä¸€åŠé€šè¿‡
            elif pass_rate > 0:
                return 1.0        # éƒ¨åˆ†é€šè¿‡
            else:
                return -5.0       # å…¨éƒ¨å¤±è´¥

        # æ–¹æ³•2: å­—ç¬¦ä¸²åŒ¹é…(åŸæœ‰æ–¹æ³•,å…œåº•)
        if ground_truth.lower() in prediction.lower():
            return 10.0

        # æ–¹æ³•3: å‡½æ•°ååŒ¹é…
        pred_funcs = re.findall(r'def\s+(\w+)\s*\(', prediction)
        gt_funcs = re.findall(r'def\s+(\w+)\s*\(', ground_truth)

        if pred_funcs and gt_funcs and pred_funcs[0] == gt_funcs[0]:
            return 5.0

        return -5.0
```

**æ•°æ®æ ¼å¼æ‰©å±•:**

éœ€è¦åœ¨æ•°æ®é›†ä¸­æ·»åŠ test_caseså­—æ®µ:
```json
{
  "problem": "Write a function to add two numbers",
  "problem_type": "code",
  "ground_truth": "def add(a, b):\n    return a + b",
  "test_cases": [
    {"assert": "assert add(2, 3) == 5"},
    {"assert": "assert add(0, 0) == 0"},
    {"assert": "assert add(-1, 1) == 0"}
  ]
}
```

**é¢„æœŸæ•ˆæœ:**
- âœ… ä»£ç é¢˜è¯„ä¼°å‡†ç¡®æ€§å¤§å¹…æå‡
- âœ… çœŸå®æ‰§è¡ŒéªŒè¯ä»£ç æ­£ç¡®æ€§
- âš ï¸  æ‰§è¡Œè€—æ—¶å¢åŠ (æ¯é¢˜5ç§’timeout)
- âš ï¸  éœ€è¦å®‰å…¨æ²™ç®±(é˜²æ­¢æ¶æ„ä»£ç )
- **å®æ–½æˆæœ¬:** é«˜(1-2å¤©)

---

### æ–¹æ¡ˆC: è‡ªé€‚åº”æƒé‡å­¦ä¹ (é«˜çº§)

**ç›®æ ‡:** è®©å¥–åŠ±å‡½æ•°æƒé‡éšè®­ç»ƒåŠ¨æ€è°ƒæ•´

**å½“å‰é—®é¢˜:** æƒé‡ç¡¬ç¼–ç ä¸º {correctness:0.7, efficiency:0.2, simplicity:0.1}

**æ”¹è¿›æ–¹æ¡ˆ:**
```python
class AdaptiveRewardWeighting:
    """è‡ªé€‚åº”å¥–åŠ±æƒé‡å­¦ä¹ """

    def __init__(self, initial_weights: Dict[str, float] = None):
        self.weights = initial_weights or {
            'correctness': 0.7,
            'efficiency': 0.2,
            'simplicity': 0.1
        }

        # æƒé‡å†å²
        self.weight_history = []

        # æ€§èƒ½æŒ‡æ ‡å†å²
        self.performance_history = []

    def update_weights(self, step: int, accuracy: float, avg_reward: float):
        """
        æ ¹æ®è®­ç»ƒæ€§èƒ½åŠ¨æ€è°ƒæ•´æƒé‡

        ç­–ç•¥:
        1. è®­ç»ƒæ—©æœŸ(step < 100): æé«˜correctnessæƒé‡
        2. è®­ç»ƒä¸­æœŸ(100-300): å¹³è¡¡ä¸‰è€…
        3. è®­ç»ƒåæœŸ(step > 300): æé«˜efficiencyæƒé‡
        """

        # è®°å½•æ€§èƒ½
        self.performance_history.append({
            'step': step,
            'accuracy': accuracy,
            'avg_reward': avg_reward
        })

        # åŠ¨æ€è°ƒæ•´ç­–ç•¥
        if step < 100:
            # æ—©æœŸ: ä¸“æ³¨æ­£ç¡®æ€§
            self.weights = {
                'correctness': 0.8,
                'efficiency': 0.1,
                'simplicity': 0.1
            }

        elif step < 300:
            # ä¸­æœŸ: å¹³è¡¡
            if accuracy < 0.5:
                # å‡†ç¡®ç‡ä½,ç»§ç»­æé«˜correctness
                self.weights['correctness'] = 0.75
                self.weights['efficiency'] = 0.15
                self.weights['simplicity'] = 0.10
            else:
                # å‡†ç¡®ç‡é«˜,å¼€å§‹ä¼˜åŒ–æ•ˆç‡
                self.weights['correctness'] = 0.65
                self.weights['efficiency'] = 0.25
                self.weights['simplicity'] = 0.10

        else:
            # åæœŸ: ä¼˜åŒ–æ•ˆç‡å’Œç®€æ´æ€§
            self.weights['correctness'] = 0.60
            self.weights['efficiency'] = 0.30
            self.weights['simplicity'] = 0.10

        # è®°å½•æƒé‡
        self.weight_history.append({
            'step': step,
            'weights': self.weights.copy()
        })

    def get_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡"""
        return self.weights.copy()

# é›†æˆåˆ°GRPOè®­ç»ƒå™¨
class GRPOTrainer:
    def __init__(self, config_path):
        # ... åŸæœ‰åˆå§‹åŒ– ...

        # æ·»åŠ è‡ªé€‚åº”æƒé‡
        self.adaptive_weights = AdaptiveRewardWeighting()

    async def train_step(self, step: int):
        # ... æ‰§è¡Œå·¥ä½œæµ,è®¡ç®—å¥–åŠ± ...

        # æ›´æ–°æƒé‡
        self.adaptive_weights.update_weights(
            step=step,
            accuracy=accuracy,
            avg_reward=np.mean(all_rewards)
        )

        # åº”ç”¨æ–°æƒé‡åˆ°reward_computer
        new_weights = self.adaptive_weights.get_weights()
        self.reward_computer.reward_weights = new_weights

        print(f"ğŸ“Š å½“å‰æƒé‡: {new_weights}")
```

**é¢„æœŸæ•ˆæœ:**
- âœ… è®­ç»ƒæ—©æœŸä¸“æ³¨å­¦ä¹ æ­£ç¡®ç­”æ¡ˆ
- âœ… è®­ç»ƒåæœŸä¼˜åŒ–æ•ˆç‡å’Œè´¨é‡
- âœ… é€‚åº”ä¸åŒè®­ç»ƒé˜¶æ®µçš„éœ€æ±‚
- **å®æ–½æˆæœ¬:** ä¸­(3-4å°æ—¶)

---

## ğŸ“‹ å®æ–½ä¼˜å…ˆçº§å’Œè·¯çº¿å›¾

### Phase 1: å¿«é€Ÿæ”¹è¿›(1å‘¨å†…)

**ä¼˜å…ˆçº§:** â­â­â­â­â­

1. **A1. æ”¹è¿›æ­£ç¡®æ€§è¯„ä¼°** (1-2å°æ—¶)
   - æ·»åŠ LaTeXè§£æ
   - æ”¯æŒæ›´å¤šæ•°å­¦è¡¨è¾¾å¼æ ¼å¼

2. **A2. æ·»åŠ æ ¼å¼å¥–åŠ±** (1å°æ—¶)
   - æ£€æŸ¥<think>/<answer>æ ‡ç­¾
   - æ£€æŸ¥ä»£ç å—æ ¼å¼

3. **A3. æ·»åŠ é‡å¤æƒ©ç½š** (1å°æ—¶)
   - N-gramé‡å¤æ£€æµ‹
   - é˜²æ­¢reward hacking

**é¢„æœŸæå‡:** å‡†ç¡®ç‡ +10-15%

---

### Phase 2: ä¸­çº§ä¼˜åŒ–(2-3å‘¨å†…)

**ä¼˜å…ˆçº§:** â­â­â­â­

4. **A4. å¯é€‰LLMè¯„åˆ¤** (2-3å°æ—¶)
   - é›†æˆGPT-4o-miniè¯„åˆ¤
   - ä»…ç”¨äºç¦»çº¿éªŒè¯

5. **C. è‡ªé€‚åº”æƒé‡å­¦ä¹ ** (3-4å°æ—¶)
   - æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´
   - è®°å½•æƒé‡å˜åŒ–å†å²

**é¢„æœŸæå‡:** å‡†ç¡®ç‡ +5-10%

---

### Phase 3: é«˜çº§åŠŸèƒ½(1-2ä¸ªæœˆ)

**ä¼˜å…ˆçº§:** â­â­â­

6. **B. ä»£ç æ‰§è¡ŒéªŒè¯** (1-2å¤©)
   - çœŸå®ä»£ç æ‰§è¡Œæ²™ç®±
   - æµ‹è¯•ç”¨ä¾‹éªŒè¯

7. **è¿‡ç¨‹å¥–åŠ±(Process Reward)** (3-5å¤©)
   - è¯„ä¼°æ¨ç†æ­¥éª¤è´¨é‡
   - Tokençº§åˆ«å¥–åŠ±è®¾è®¡

**é¢„æœŸæå‡:** å‡†ç¡®ç‡ +10-20% (å°¤å…¶æ˜¯ä»£ç é¢˜)

---

## ğŸ’¡ ç«‹å³å¯å®æ–½çš„Quick Wins

### 1. è°ƒæ•´å½“å‰æƒé‡(0æˆæœ¬)

**å»ºè®®ä¿®æ”¹ `config/training.yaml`:**
```yaml
reward_weights:
  correctness: 0.75    # ä»0.7æå‡åˆ°0.75
  efficiency: 0.15     # ä»0.2é™ä½åˆ°0.15
  simplicity: 0.10     # ä¿æŒ0.1
```

**ç†ç”±:** è®­ç»ƒåˆæœŸåº”æ›´é‡è§†æ­£ç¡®æ€§

---

### 2. é™ä½correctnessé˜ˆå€¼(10åˆ†é’Ÿ)

**ä¿®æ”¹ `src/reward_computer.py`:**
```python
# å½“å‰é˜ˆå€¼è¿‡ä¸¥
def _compute_math_correctness(self, prediction: str, ground_truth: str) -> float:
    # æ—§ä»£ç 
    if abs(pred_answer - gt_answer) < 1e-4:
        return 10.0
    elif abs(pred_answer - gt_answer) < 1.0:    # é˜ˆå€¼1.0
        return 5.0
    else:
        return -5.0

# æ”¹è¿›å»ºè®®
def _compute_math_correctness(self, prediction: str, ground_truth: str) -> float:
    diff = abs(pred_answer - gt_answer)

    if diff < 1e-4:
        return 10.0       # å®Œå…¨æ­£ç¡®
    elif diff < 0.1:      # æ–°å¢: éå¸¸æ¥è¿‘
        return 8.0
    elif diff < 1.0:
        return 5.0        # æ¥è¿‘
    elif diff < 10.0:     # æ–°å¢: æ•°é‡çº§æ­£ç¡®
        return 2.0
    else:
        return -5.0       # å®Œå…¨é”™è¯¯
```

**é¢„æœŸæ•ˆæœ:** å¯¹æ¥è¿‘æ­£ç¡®çš„ç­”æ¡ˆç»™äºˆéƒ¨åˆ†å¥–åŠ±,åŠ é€Ÿå­¦ä¹ 

---

## ğŸ“Š ç›‘æ§å’ŒéªŒè¯

**æ·»åŠ è¯¦ç»†çš„å¥–åŠ±åˆ†è§£æ—¥å¿—:**

```python
# åœ¨grpo_trainer.pyä¸­
print(f"""
ğŸ¯ å¥–åŠ±åˆ†è§£:
  - æ­£ç¡®æ€§: {correctness:.2f}/10.0 (æƒé‡70%)
  - æ•ˆç‡:   {efficiency:.2f}/10.0 (æƒé‡20%)
  - ç®€æ´æ€§: {simplicity:.2f}/10.0 (æƒé‡10%)
  - æ€»å¥–åŠ±: {total_reward:.2f}/10.0
""")
```

**æ·»åŠ å¥–åŠ±åˆ†å¸ƒç»Ÿè®¡:**
```python
# åœ¨analyze_training.pyä¸­
def analyze_reward_distribution(self):
    """åˆ†æå„ç»´åº¦å¥–åŠ±åˆ†å¸ƒ"""

    # æå–å„ç»´åº¦åˆ†æ•°
    correctness_scores = re.findall(r'æ­£ç¡®æ€§: ([\d.-]+)/10\.0', content)
    efficiency_scores = re.findall(r'æ•ˆç‡: ([\d.-]+)/10\.0', content)

    print(f"\nğŸ“Š å¥–åŠ±åˆ†å¸ƒ:")
    print(f"  æ­£ç¡®æ€§: Î¼={np.mean(correctness):.2f}, Ïƒ={np.std(correctness):.2f}")
    print(f"  æ•ˆç‡:   Î¼={np.mean(efficiency):.2f}, Ïƒ={np.std(efficiency):.2f}")
```

---

## ğŸ¯ æ€»ç»“

### å½“å‰ç³»ç»Ÿä¼˜åŠ¿
- âœ… ç®€æ´é«˜æ•ˆ(185è¡Œä»£ç )
- âœ… é€‚åˆåœ¨çº¿RLè®­ç»ƒ
- âœ… å¤šç»´åº¦è¦†ç›–(ç»“æœ+æ•ˆç‡+è´¨é‡)

### ä¸»è¦æ”¹è¿›æ–¹å‘
1. **æ­£ç¡®æ€§è¯„ä¼°** - å€Ÿé‰´ROLLçš„math_verifyå’ŒLaTeXè§£æ
2. **æ ¼å¼è§„èŒƒ** - æ·»åŠ æ ¼å¼æ£€æŸ¥å¥–åŠ±ç»´åº¦
3. **é˜²reward hacking** - æ·»åŠ é‡å¤æƒ©ç½š
4. **ä»£ç éªŒè¯** - å¯é€‰çš„çœŸå®æ‰§è¡Œæµ‹è¯•
5. **è‡ªé€‚åº”æƒé‡** - æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´

### æ¨èå®æ–½é¡ºåº
```
Phase 1 (ç«‹å³): A1 + A2 + A3 â†’ é¢„æœŸå‡†ç¡®ç‡ 37.5% â†’ 50%+
Phase 2 (1å‘¨): A4 + C â†’ é¢„æœŸå‡†ç¡®ç‡ 50% â†’ 60%+
Phase 3 (1æœˆ): B + è¿‡ç¨‹å¥–åŠ± â†’ é¢„æœŸå‡†ç¡®ç‡ 60% â†’ 75%+
```

### é£é™©å’Œæ³¨æ„äº‹é¡¹
- âš ï¸  ä»£ç æ‰§è¡Œéœ€è¦å®‰å…¨æ²™ç®±(docker/firejail)
- âš ï¸  LLMè¯„åˆ¤æˆæœ¬é«˜,ä»…ç”¨äºéªŒè¯
- âš ï¸  æƒé‡è°ƒæ•´éœ€è¦A/Bæµ‹è¯•éªŒè¯æ•ˆæœ
- âš ï¸  è¿‡ç¨‹å¥–åŠ±è®¾è®¡å¤æ‚,éœ€è¦å¤§é‡å®éªŒ

---

**æ–‡æ¡£ç‰ˆæœ¬:** v1.0
**åˆ›å»ºæ—¶é—´:** 2025-11-16
**ä¸‹æ¬¡æ›´æ–°:** å®æ–½Phase 1å
