#!/usr/bin/env python3
"""
æ•°æ®å‡†å¤‡è„šæœ¬ - ä»ROLLå’ŒAFlowæå–å’Œæ•´åˆæ•°æ®é›†
"""
import json
import sys
import os
from pathlib import Path
from typing import List, Dict, Optional
import random

# æ·»åŠ è·¯å¾„
sys.path.append('/home/yijia/.claude/11/ROLL')
sys.path.append('/home/yijia/.claude/11/AFlow')

class DataPreparer:
    """æ•°æ®å‡†å¤‡å™¨ï¼šæ•´åˆROLLå’ŒAFlowçš„æ•°æ®é›†"""

    def __init__(self, output_dir: str = "data"):
        self.output_dir = Path(output_dir)
        self.aflow_path = Path('/home/yijia/.claude/11/AFlow')
        self.roll_path = Path('/home/yijia/.claude/11/ROLL')

        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "train").mkdir(exist_ok=True)
        (self.output_dir / "val").mkdir(exist_ok=True)
        (self.output_dir / "test").mkdir(exist_ok=True)

    def extract_aflow_benchmark_data(self, benchmark_name: str) -> List[Dict]:
        """ä»AFlowæå–benchmarkæ•°æ®"""
        data = []

        try:
            # åŠ¨æ€å¯¼å…¥AFlowçš„benchmark
            sys.path.insert(0, str(self.aflow_path))

            if benchmark_name == "gsm8k":
                from benchmarks.gsm8k import GSM8KBenchmark
                benchmark = GSM8KBenchmark()
                dataset_type = "math"
            elif benchmark_name == "math":
                from benchmarks.math import MATHBenchmark
                benchmark = MATHBenchmark()
                dataset_type = "math"
            elif benchmark_name == "humaneval":
                from benchmarks.humaneval import HumanEvalBenchmark
                benchmark = HumanEvalBenchmark()
                dataset_type = "code"
            elif benchmark_name == "mbpp":
                from benchmarks.mbpp import MBPPBenchmark
                benchmark = MBPPBenchmark()
                dataset_type = "code"
            elif benchmark_name == "hotpotqa":
                from benchmarks.hotpotqa import HotpotQABenchmark
                benchmark = HotpotQABenchmark()
                dataset_type = "qa"
            elif benchmark_name == "drop":
                from benchmarks.drop import DROPBenchmark
                benchmark = DROPBenchmark()
                dataset_type = "qa"
            else:
                print(f"âš ï¸  æœªçŸ¥çš„benchmark: {benchmark_name}")
                return []

            # åŠ è½½æ•°æ®
            import asyncio
            raw_data = asyncio.run(benchmark.load_data(split="all"))

            # è½¬æ¢æ ¼å¼
            for item in raw_data:
                sample = {
                    "problem": self._extract_problem(item, dataset_type),
                    "problem_type": dataset_type,
                    "tag": benchmark_name,
                    "ground_truth": self._extract_ground_truth(item, dataset_type),
                    "metadata": item
                }

                # ä»£ç ç±»å‹éœ€è¦é¢å¤–å­—æ®µ
                if dataset_type == "code":
                    sample["entry_point"] = item.get("entry_point", "")
                    sample["test_code"] = item.get("test", "")

                data.append(sample)

            print(f"âœ… ä» {benchmark_name} æå– {len(data)} ä¸ªæ ·æœ¬")

        except Exception as e:
            print(f"âŒ æå– {benchmark_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        return data

    def _extract_problem(self, item: Dict, dataset_type: str) -> str:
        """æå–é—®é¢˜æ–‡æœ¬"""
        if "prompt" in item:
            return item["prompt"]
        elif "question" in item:
            return item["question"]
        elif "problem" in item:
            return item["problem"]
        else:
            return str(item.get("input", ""))

    def _extract_ground_truth(self, item: Dict, dataset_type: str) -> str:
        """æå–æ­£ç¡®ç­”æ¡ˆ"""
        if "answer" in item:
            return str(item["answer"])
        elif "code" in item:
            return item["code"]
        elif "canonical_solution" in item:
            return item["canonical_solution"]
        else:
            return str(item.get("output", ""))

    def create_mixed_dataset(
        self,
        math_ratio: float = 0.4,
        code_ratio: float = 0.3,
        qa_ratio: float = 0.3,
        total_train: int = 10000,
        total_val: int = 1000,
        total_test: int = 500
    ):
        """åˆ›å»ºæ··åˆæ•°æ®é›†"""
        print("=" * 60)
        print("ğŸ“¦ åˆ›å»ºæ··åˆæ•°æ®é›†")
        print("=" * 60)

        # ä»AFlowæå–æ•°æ®
        print("\nğŸ” ä»AFlowæå–æ•°æ®...")

        math_data = []
        math_data.extend(self.extract_aflow_benchmark_data("gsm8k"))
        math_data.extend(self.extract_aflow_benchmark_data("math"))

        code_data = []
        code_data.extend(self.extract_aflow_benchmark_data("humaneval"))
        code_data.extend(self.extract_aflow_benchmark_data("mbpp"))

        qa_data = []
        qa_data.extend(self.extract_aflow_benchmark_data("hotpotqa"))
        qa_data.extend(self.extract_aflow_benchmark_data("drop"))

        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ•°å­¦: {len(math_data)} æ ·æœ¬")
        print(f"  ä»£ç : {len(code_data)} æ ·æœ¬")
        print(f"  QA: {len(qa_data)} æ ·æœ¬")

        # æ´—ç‰Œ
        random.shuffle(math_data)
        random.shuffle(code_data)
        random.shuffle(qa_data)

        # æŒ‰æ¯”ä¾‹åˆ†é…
        def create_split(total: int, split_name: str):
            """åˆ›å»ºæ•°æ®é›†åˆ†å‰²"""
            math_count = int(total * math_ratio)
            code_count = int(total * code_ratio)
            qa_count = total - math_count - code_count

            split_data = []

            # é‡‡æ ·æ•°å­¦æ•°æ®
            if len(math_data) >= math_count:
                split_data.extend(random.sample(math_data, math_count))
            else:
                print(f"âš ï¸  æ•°å­¦æ•°æ®ä¸è¶³ï¼Œéœ€è¦{math_count}ï¼Œåªæœ‰{len(math_data)}")
                split_data.extend(math_data)

            # é‡‡æ ·ä»£ç æ•°æ®
            if len(code_data) >= code_count:
                split_data.extend(random.sample(code_data, code_count))
            else:
                print(f"âš ï¸  ä»£ç æ•°æ®ä¸è¶³ï¼Œéœ€è¦{code_count}ï¼Œåªæœ‰{len(code_data)}")
                split_data.extend(code_data)

            # é‡‡æ ·QAæ•°æ®
            if len(qa_data) >= qa_count:
                split_data.extend(random.sample(qa_data, qa_count))
            else:
                print(f"âš ï¸  QAæ•°æ®ä¸è¶³ï¼Œéœ€è¦{qa_count}ï¼Œåªæœ‰{len(qa_data)}")
                split_data.extend(qa_data)

            # æ´—ç‰Œ
            random.shuffle(split_data)

            # ä¿å­˜
            output_file = self.output_dir / f"{split_name}/mixed_dataset.jsonl"
            with open(output_file, 'w') as f:
                for sample in split_data:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\n')

            print(f"âœ… {split_name.upper()} æ•°æ®é›†: {len(split_data)} æ ·æœ¬ -> {output_file}")

            return split_data

        # åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
        train_data = create_split(total_train, "train")
        val_data = create_split(total_val, "val")
        test_data = create_split(total_test, "test")

        print("\n" + "=" * 60)
        print("âœ… æ··åˆæ•°æ®é›†åˆ›å»ºå®Œæˆ")
        print("=" * 60)

        return {
            "train": train_data,
            "val": val_data,
            "test": test_data
        }

    def verify_dataset(self, split: str = "train"):
        """éªŒè¯æ•°æ®é›†æ ¼å¼"""
        print(f"\nğŸ” éªŒè¯ {split.upper()} æ•°æ®é›†...")

        dataset_file = self.output_dir / f"{split}/mixed_dataset.jsonl"

        if not dataset_file.exists():
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
            return False

        samples = []
        with open(dataset_file, 'r') as f:
            for line in f:
                if line.strip():
                    samples.append(json.loads(line))

        print(f"âœ… å…±åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")

        # éªŒè¯æ ¼å¼
        required_fields = ["problem", "problem_type", "tag", "ground_truth"]

        for i, sample in enumerate(samples[:5]):  # æ£€æŸ¥å‰5ä¸ª
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜ç±»å‹: {sample.get('problem_type')}")
            print(f"  æ ‡ç­¾: {sample.get('tag')}")
            print(f"  é—®é¢˜: {sample.get('problem', '')[:100]}...")
            print(f"  ç­”æ¡ˆ: {sample.get('ground_truth', '')[:100]}...")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            for field in required_fields:
                if field not in sample:
                    print(f"  âŒ ç¼ºå°‘å­—æ®µ: {field}")

        # ç»Ÿè®¡
        type_counts = {}
        for sample in samples:
            ptype = sample.get('problem_type', 'unknown')
            type_counts[ptype] = type_counts.get(ptype, 0) + 1

        print(f"\nğŸ“Š ç±»å‹åˆ†å¸ƒ:")
        for ptype, count in type_counts.items():
            ratio = count / len(samples) * 100
            print(f"  {ptype}: {count} ({ratio:.1f}%)")

        return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å‡†å¤‡æ··åˆæ•°æ®é›†")
    parser.add_argument('--output-dir', type=str, default='data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--train-size', type=int, default=10000, help='è®­ç»ƒé›†å¤§å°')
    parser.add_argument('--val-size', type=int, default=1000, help='éªŒè¯é›†å¤§å°')
    parser.add_argument('--test-size', type=int, default=500, help='æµ‹è¯•é›†å¤§å°')
    parser.add_argument('--math-ratio', type=float, default=0.4, help='æ•°å­¦æ•°æ®æ¯”ä¾‹')
    parser.add_argument('--code-ratio', type=float, default=0.3, help='ä»£ç æ•°æ®æ¯”ä¾‹')
    parser.add_argument('--qa-ratio', type=float, default=0.3, help='QAæ•°æ®æ¯”ä¾‹')
    parser.add_argument('--verify-only', action='store_true', help='ä»…éªŒè¯ç°æœ‰æ•°æ®é›†')

    args = parser.parse_args()

    preparer = DataPreparer(output_dir=args.output_dir)

    if args.verify_only:
        preparer.verify_dataset("train")
        preparer.verify_dataset("val")
        preparer.verify_dataset("test")
    else:
        preparer.create_mixed_dataset(
            math_ratio=args.math_ratio,
            code_ratio=args.code_ratio,
            qa_ratio=args.qa_ratio,
            total_train=args.train_size,
            total_val=args.val_size,
            total_test=args.test_size
        )

        # éªŒè¯
        preparer.verify_dataset("train")


if __name__ == "__main__":
    main()
