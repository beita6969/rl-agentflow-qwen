#!/usr/bin/env python3
"""
ä¸‹è½½Qwen2.5-7Bæ¨¡å‹
"""
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

def download_model():
    """ä¸‹è½½Qwen2.5-7Bæ¨¡å‹åˆ°ç¼“å­˜"""

    print("=" * 60)
    print("ğŸ“¥ ä¸‹è½½Qwen2.5-7B-Instructæ¨¡å‹")
    print("=" * 60)

    model_name = "Qwen/Qwen2.5-7B-Instruct"

    # è®¾ç½®HuggingFace token (ä»ç¯å¢ƒå˜é‡è¯»å–)
    token = os.environ.get('HF_TOKEN', None)
    if not token:
        print("âš ï¸  è­¦å‘Š: æœªè®¾ç½®HF_TOKENç¯å¢ƒå˜é‡")
        print("   è¯·è¿è¡Œ: export HF_TOKEN='your_token_here'")
        print("   æˆ–åœ¨ä»£ç ä¸­è®¾ç½® token å˜é‡")

    try:
        print(f"\nğŸ“¥ ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            token=token
        )
        print("âœ… Tokenizerä¸‹è½½å®Œæˆ")

        print(f"\nğŸ“¥ ä¸‹è½½æ¨¡å‹ï¼ˆçº¦14GBï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="cpu",  # å…ˆä¸‹è½½åˆ°CPU
            trust_remote_code=True,
            token=token
        )
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")

        # æ˜¾ç¤ºç¼“å­˜è·¯å¾„
        from transformers.utils import TRANSFORMERS_CACHE
        print(f"\nğŸ“‚ æ¨¡å‹ç¼“å­˜è·¯å¾„: {TRANSFORMERS_CACHE}")

        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æ–‡ä»¶ä¸‹è½½å®Œæˆï¼")
        print("=" * 60)

        return True

    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„åŸå› :")
        print("  1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("  2. HuggingFace tokenæ— æ•ˆ")
        print("  3. ç£ç›˜ç©ºé—´ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘20GBï¼‰")
        return False

if __name__ == "__main__":
    download_model()
